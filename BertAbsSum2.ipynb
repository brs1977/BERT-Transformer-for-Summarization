{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "BertAbsSum2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "U7h5IEsJfJs7"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brs1977/BERT-Transformer-for-Summarization/blob/master/BertAbsSum2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DKufDQceuaD",
        "colab_type": "code",
        "outputId": "a297801f-5e3e-4bcb-ea7f-b663b492d581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7h5IEsJfJs7",
        "colab_type": "text"
      },
      "source": [
        "##Get Kaggle data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ7V-uhSfIYk",
        "colab_type": "code",
        "outputId": "458c0b73-0702-43d9-dd9e-c183079ec03b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#!pip install -q kaggle\n",
        "\n",
        "#kaggle key\n",
        "!mkdir ~/.kaggle\n",
        "!cp /content/drive/My\\ Drive/kaggle.json ~/.kaggle\n",
        "!ls ~/.kaggle"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h32t1QHfQk8",
        "colab_type": "code",
        "outputId": "7a98563c-09d5-4272-dbe6-d12e0d9068a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "!kaggle competitions download -c title-generation"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading vocs.pkl.zip to /content\n",
            " 78% 5.00M/6.39M [00:00<00:00, 45.3MB/s]\n",
            "100% 6.39M/6.39M [00:00<00:00, 41.0MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/778k [00:00<?, ?B/s]\n",
            "100% 778k/778k [00:00<00:00, 244MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 80% 35.0M/43.7M [00:00<00:00, 36.7MB/s]\n",
            "100% 43.7M/43.7M [00:00<00:00, 74.6MB/s]\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/905k [00:00<?, ?B/s]\n",
            "100% 905k/905k [00:00<00:00, 126MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpltIhc-fVqS",
        "colab_type": "code",
        "outputId": "8ecb5150-7c24-4ca1-8386-ba66c89f77ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!mkdir data\n",
        "!unzip sample_submission.csv.zip -d data\n",
        "!unzip vocs.pkl.zip -d data\n",
        "!unzip train.csv.zip -d data\n",
        "!mv test.csv data\n",
        "\n",
        "!ls data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: data/sample_submission.csv  \n",
            "Archive:  vocs.pkl.zip\n",
            "  inflating: data/vocs.pkl           \n",
            "Archive:  train.csv.zip\n",
            "  inflating: data/train.csv          \n",
            "sample_submission.csv  test.csv  train.csv  vocs.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiawlFnGfbV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv('data/train.csv', encoding='utf8')\n",
        "test = pd.read_csv('data/test.csv', encoding='utf8')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0288Y3gPfc8a",
        "colab_type": "code",
        "outputId": "e0b8b8a7-b169-4d48-e77d-45b6051d3eb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>we consider the problem of utility maximizatio...</td>\n",
              "      <td>on optimal investment with processes of long o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>in this paper we provide an explicit formula f...</td>\n",
              "      <td>boolean complexes for ferrers graphs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>kinesin-5, also known as eg5 in vertebrates is...</td>\n",
              "      <td>relative velocity of sliding of microtubules b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>we discuss the transition paths in a coupled b...</td>\n",
              "      <td>bifurcation of transition paths induced by cou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>two types of room temperature detectors of ter...</td>\n",
              "      <td>all-electric detectors of the polarization sta...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            abstract                                              title\n",
              "0  we consider the problem of utility maximizatio...  on optimal investment with processes of long o...\n",
              "1  in this paper we provide an explicit formula f...               boolean complexes for ferrers graphs\n",
              "2  kinesin-5, also known as eg5 in vertebrates is...  relative velocity of sliding of microtubules b...\n",
              "3  we discuss the transition paths in a coupled b...  bifurcation of transition paths induced by cou...\n",
              "4  two types of room temperature detectors of ter...  all-electric detectors of the polarization sta..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CjNvi4nfhFc",
        "colab_type": "text"
      },
      "source": [
        "##Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWO9FdL0fB5v",
        "colab_type": "code",
        "outputId": "c1cad62f-7628-4ec6-dd00-ca0e86c3751b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "%cd /content\n",
        "!rm bertsum -r\n",
        "!git clone https://github.com/brs1977/BERT-Transformer-for-Summarization bertsum\n",
        "\n",
        "# !git pull origin master"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "rm: cannot remove 'bertsum': No such file or directory\n",
            "Cloning into 'bertsum'...\n",
            "remote: Enumerating objects: 105, done.\u001b[K\n",
            "remote: Counting objects: 100% (105/105), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 105 (delta 47), reused 76 (delta 27), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (105/105), 107.04 KiB | 3.15 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq77ji8Wow4T",
        "colab_type": "code",
        "outputId": "12ce746c-22fa-4ba9-f253-9690f1cce049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/bertsum"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/bertsum\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvZ-_YvEeQLk",
        "colab_type": "code",
        "outputId": "b2f11bf3-bdc1-4b43-a1d7-4459cf44d599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/bertsum\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "from preprocess import CSVProcessor, create_dataset\n",
        "from model import BertAbsSum\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from pytorch_pretrained_bert.optimization import BertAdam\n",
        "from preprocess import convert_examples_to_features\n",
        "from tqdm import tqdm, trange\n",
        "from transformer import Constants"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/bertsum\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYvyMXe7f69f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB-lgaRTf7A1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBJiX709huHD",
        "colab_type": "text"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuFtKU0eiDwi",
        "colab_type": "text"
      },
      "source": [
        "###Bert config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRj0gX06eQLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                    level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uelhDbCjeQLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQBqPZK9eQLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ARGS(object):\n",
        "    data_dir = 'data/processed_data'\n",
        "    bert_model = 'bert-base-uncased'\n",
        "    #output_dir = 'output'\n",
        "    output_dir = '/content/drive/My Drive/nlp'\n",
        "    GPU_index = 0\n",
        "    learning_rate = 5e-3\n",
        "    num_train_epochs = 3\n",
        "    warmup_proportion = 0.1\n",
        "    max_src_len = 130\n",
        "    max_tgt_len = 30\n",
        "    train_batch_size = 32\n",
        "    valid_batch_size = 32\n",
        "    decoder_config = None\n",
        "    print_every = 100\n",
        "    gradient_accumulation_steps = 1\n",
        "\n",
        "\n",
        "args = ARGS()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH4VXzObeQLx",
        "colab_type": "code",
        "outputId": "c96a2632-491a-4b71-bce2-c0261a0de66d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda', args.GPU_index)\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "logger.info(f'Using device:{device}')\n",
        "\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)\n",
        "model_path = os.path.join(args.output_dir, time.strftime('model_%m-%d-%H:%M:%S', time.localtime()))\n",
        "os.mkdir(model_path)\n",
        "logger.info(f'Saving model to {model_path}.')\n",
        "\n",
        "if args.decoder_config is not None:\n",
        "    with open(args.decoder_config, 'r') as f:\n",
        "        decoder_config = json.load(f)\n",
        "else:\n",
        "    with open(os.path.join(args.bert_model, 'bert_config.json'), 'r') as f:\n",
        "        bert_config = json.load(f)\n",
        "        decoder_config = {}\n",
        "        decoder_config = {}\n",
        "        decoder_config['len_max_seq'] = args.max_tgt_len\n",
        "        decoder_config['d_word_vec'] = bert_config['hidden_size']\n",
        "        decoder_config['n_layers'] = 8\n",
        "        decoder_config['n_head'] = 12\n",
        "        decoder_config['d_k'] = 64\n",
        "        decoder_config['d_v'] = 64\n",
        "        decoder_config['d_model'] = bert_config['hidden_size']\n",
        "        decoder_config['d_inner'] = bert_config['hidden_size']\n",
        "        decoder_config['vocab_size'] = bert_config['vocab_size']        "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01/23/2020 08:06:16 - INFO - __main__ -   Using device:cuda:0\n",
            "01/23/2020 08:06:16 - INFO - __main__ -   Saving model to /content/drive/My Drive/nlp/model_01-23-08:06:16.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE6VE9npiQ6t",
        "colab_type": "text"
      },
      "source": [
        "###Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGqDLLzJO1hq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "58d7fc8d-8d05-41b4-849b-0d5be04b3e7c"
      },
      "source": [
        "#prepared data\n",
        "train_data  = torch.load('/content/drive/My Drive/nlp/nlp_model/abs_bert/data130X30/train.pt')\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size, drop_last=True)\n",
        "\n",
        "valid_data = torch.load('/content/drive/My Drive/nlp/nlp_model/abs_bert/data130X30/valid.pt')\n",
        "valid_sampler = RandomSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=args.valid_batch_size, drop_last=True)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
        "\n",
        "\n",
        "# torch.save(train_data,'train.pt')\n",
        "# torch.save(valid_data,'valid.pt')\n",
        "# !cp *.pt /content/drive/My\\ Drive/nlp/nlp_model/abs_bert/data130X30\n",
        "# !ls /content/drive/My\\ Drive/nlp/nlp_model/abs_bert/data130X30 -la\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01/23/2020 08:07:09 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uok4w6IzVo7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "is_test = False\n",
        "nrows = None if not is_test else 500\n",
        "\n",
        "# data preprocess\n",
        "processor = CSVProcessor()\n",
        "tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
        "# tokenizer = BertTokenizer.from_pretrained(os.path.join(args.bert_model, 'vocab.txt'))\n",
        "logger.info('Loading train examples...')\n",
        "train_examples = processor.get_train_examples('../data/train.csv', nrows = nrows)\n",
        "num_train_optimization_steps = int(len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n",
        "logger.info('Converting train examples to features...')\n",
        "train_features = convert_examples_to_features(train_examples, args.max_src_len, args.max_tgt_len, tokenizer)\n",
        "example = train_examples[0]\n",
        "example_feature = train_features[0]\n",
        "logger.info(\"*** Example ***\")\n",
        "logger.info(\"guid: %s\" % (example.guid))\n",
        "logger.info(\"src text: %s\" % example.src)\n",
        "logger.info(\"src_ids: %s\" % \" \".join([str(x) for x in example_feature.src_ids]))\n",
        "logger.info(\"src_mask: %s\" % \" \".join([str(x) for x in example_feature.src_mask]))\n",
        "logger.info(\"tgt text: %s\" % example.tgt)\n",
        "logger.info(\"tgt_ids: %s\" % \" \".join([str(x) for x in example_feature.tgt_ids]))\n",
        "logger.info(\"tgt_mask: %s\" % \" \".join([str(x) for x in example_feature.tgt_mask]))\n",
        "logger.info('Building dataloader...')\n",
        "train_data = create_dataset(train_features)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size, drop_last=True)\n",
        "\n",
        "valid_examples = processor.get_valid_examples('../data/train.csv')\n",
        "logger.info('Converting valid examples to features...')\n",
        "valid_features = convert_examples_to_features(valid_examples, args.max_src_len, args.max_tgt_len, tokenizer)\n",
        "valid_data = create_dataset(valid_features)\n",
        "valid_sampler = RandomSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=args.train_batch_size, drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgeH3YrqWuvI",
        "colab_type": "code",
        "outputId": "88c8f9d2-a29d-4d0c-d1ca-4f3f1c604ca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8973"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNny66w7ioT7",
        "colab_type": "text"
      },
      "source": [
        "###Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx8bd6jR3C_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_performance(logits, ground, smoothing=True):\n",
        "    ground = ground[:, 1:]\n",
        "    logits = logits.view(-1, logits.size(-1))\n",
        "    ground = ground.contiguous().view(-1)\n",
        "\n",
        "    loss = cal_loss(logits, ground, smoothing=smoothing)\n",
        "\n",
        "    pad_mask = ground.ne(Constants.PAD)\n",
        "    pred = logits.max(-1)[1]\n",
        "    correct = pred.eq(ground)\n",
        "    correct = correct.masked_select(pad_mask).sum().item()\n",
        "    return loss, correct\n",
        "\n",
        "def cal_loss(logits, ground, smoothing=True):\n",
        "    def label_smoothing(logits, labels):\n",
        "        eps = 0.1\n",
        "        num_classes = logits.size(-1)\n",
        "\n",
        "        # >>> z = torch.zeros(2, 4).scatter_(1, torch.tensor([[2], [3]]), 1.23)\n",
        "        # >>> z\n",
        "        # tensor([[ 0.0000,  0.0000,  1.2300,  0.0000],\n",
        "        #        [ 0.0000,  0.0000,  0.0000,  1.2300]])\n",
        "        one_hot = torch.zeros_like(logits).scatter(1, labels.view(-1, 1), 1)\n",
        "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (num_classes - 1)\n",
        "        log_prb = F.log_softmax(logits, dim=1)\n",
        "        non_pad_mask = ground.ne(Constants.PAD)\n",
        "        loss = -(one_hot * log_prb).sum(dim=1)\n",
        "        loss = loss.masked_select(non_pad_mask).mean()\n",
        "        return loss\n",
        "    if smoothing:\n",
        "        loss = label_smoothing(logits, ground)\n",
        "    else:\n",
        "        loss = F.cross_entropy(logits, ground, ignore_index=Constants.PAD)\n",
        "    \n",
        "    return loss    \n",
        "\n",
        "def rouge(hyp, ref, n):\n",
        "    scores = []\n",
        "    for h, r in zip(hyp, ref):\n",
        "        r = re.sub(r'[UNK]', '', r)\n",
        "        r = re.sub(r'[’!\"#$%&\\'()*+,-./:：？！《》;<=>?@[\\\\]^_`{|}~]+', '', r)\n",
        "        r = re.sub(r'\\d', '', r)\n",
        "        r = re.sub(r'[a-zA-Z]', '', r)\n",
        "        count = 0\n",
        "        match = 0\n",
        "        for i in range(len(r) - n):\n",
        "            gram = r[i:i + n]\n",
        "            if gram in h:\n",
        "                match += 1\n",
        "            count += 1\n",
        "        scores.append(0 if count==0 else match / count)\n",
        "    return np.average(scores)\n",
        "\n",
        "def convert_one_example(text, src_max_seq_length, tokenizer):\n",
        "    src_tokens = tokenizer.tokenize(text)\n",
        "    if len(src_tokens) > src_max_seq_length - 2:\n",
        "        src_tokens = src_tokens[:(src_max_seq_length - 2)]\n",
        "    src_tokens = [\"[CLS]\"] + src_tokens + [\"[SEP]\"]\n",
        "\n",
        "    src_ids = tokenizer.convert_tokens_to_ids(src_tokens)\n",
        "\n",
        "    src_mask = [1] * len(src_ids)\n",
        "    src_padding = [0] * (src_max_seq_length - len(src_ids))\n",
        "    src_ids += src_padding\n",
        "    src_mask += src_padding\n",
        "\n",
        "    return torch.tensor([src_ids]), torch.tensor([src_mask])    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1dhUYPgjhW3",
        "colab_type": "text"
      },
      "source": [
        "###Validation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSNeu5s1jlGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "def do_validate():\n",
        "  logger.info(\"***** Running validation *****\")\n",
        "  model.eval()\n",
        "  hyp_list = []\n",
        "  ref_list = []\n",
        "  i = 0\n",
        "  for batch in tqdm(valid_dataloader, desc=\"Val iter\", position=0):\n",
        "      i += 1\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      pred, _ = model.beam_decode(batch[0], batch[1], 3, 3)\n",
        "      src, tgt = batch[0], batch[2]\n",
        "      for i in range(args.train_batch_size):\n",
        "          sample_src = \"\".join(tokenizer.convert_ids_to_tokens(src[i].cpu().numpy())).split('[CLS]')[1].split('[SEP]')[0] + '\\n'\n",
        "          sample_tgt = \"\".join(tokenizer.convert_ids_to_tokens(tgt[i].cpu().numpy())).split('[CLS]')[1].split('[SEP]')[0] + '\\n'\n",
        "          sample_pred = \"\".join(tokenizer.convert_ids_to_tokens(pred[i][0])).split('[SEP]')[0] + '\\n'\n",
        "\n",
        "          hyp_list.append(sample_pred)\n",
        "          ref_list.append(sample_tgt)\n",
        "  rouge_1 = rouge(hyp_list, ref_list, 1)\n",
        "  rouge_2 = rouge(hyp_list, ref_list, 2)\n",
        "  logger.info('******Validation results******')\n",
        "  logger.info(f'Rouge-1: {rouge_1}')\n",
        "  logger.info(f'Rouge-2: {rouge_2}')\n",
        "  logger.info('Validation finished.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtGK6INWjlKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLh3CyBgiZKB",
        "colab_type": "text"
      },
      "source": [
        "###Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7Q9fMBjsBHP",
        "colab_type": "code",
        "outputId": "e412a93b-7053-45a7-bf45-dc83007c8462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# model\n",
        "model = BertAbsSum(args.bert_model, decoder_config, device=device )\n",
        "model.to(device)\n",
        "\n",
        "# optimizer\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=args.learning_rate,\n",
        "                     warmup=0.1,\n",
        "                     t_total=num_train_optimization_steps)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01/23/2020 08:07:30 - INFO - pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz not found in cache, downloading to /tmp/tmpsi91l5g7\n",
            "\n",
            "  0%|          | 0/407873900 [00:00<?, ?B/s]\u001b[A\n",
            "  0%|          | 261120/407873900 [00:00<02:57, 2294478.75B/s]\u001b[A\n",
            "  1%|          | 2252800/407873900 [00:00<02:09, 3123598.44B/s]\u001b[A\n",
            "  2%|▏         | 9467904/407873900 [00:00<01:30, 4380987.23B/s]\u001b[A\n",
            "  4%|▍         | 17385472/407873900 [00:00<01:03, 6113565.30B/s]\u001b[A\n",
            "  6%|▌         | 25232384/407873900 [00:00<00:45, 8451455.75B/s]\u001b[A\n",
            "  8%|▊         | 33085440/407873900 [00:00<00:32, 11540938.80B/s]\u001b[A\n",
            " 10%|▉         | 40591360/407873900 [00:00<00:23, 15464892.92B/s]\u001b[A\n",
            " 12%|█▏        | 48996352/407873900 [00:00<00:17, 20474959.78B/s]\u001b[A\n",
            " 14%|█▍        | 57249792/407873900 [00:00<00:13, 26438880.17B/s]\u001b[A\n",
            " 16%|█▌        | 64813056/407873900 [00:01<00:10, 32847978.49B/s]\u001b[A\n",
            " 18%|█▊        | 73461760/407873900 [00:01<00:08, 40356431.18B/s]\u001b[A\n",
            " 20%|██        | 81846272/407873900 [00:01<00:06, 47792733.75B/s]\u001b[A\n",
            " 22%|██▏       | 90265600/407873900 [00:01<00:05, 54914882.87B/s]\u001b[A\n",
            " 24%|██▍       | 98497536/407873900 [00:01<00:05, 61007115.24B/s]\u001b[A\n",
            " 26%|██▌       | 106649600/407873900 [00:01<00:04, 63247688.85B/s]\u001b[A\n",
            " 28%|██▊       | 114525184/407873900 [00:01<00:04, 67218102.67B/s]\u001b[A\n",
            " 30%|██▉       | 122335232/407873900 [00:01<00:04, 62281180.94B/s]\u001b[A\n",
            " 32%|███▏      | 130145280/407873900 [00:01<00:04, 66309563.06B/s]\u001b[A\n",
            " 34%|███▎      | 137432064/407873900 [00:01<00:04, 67231879.32B/s]\u001b[A\n",
            " 35%|███▌      | 144617472/407873900 [00:02<00:03, 68271299.88B/s]\u001b[A\n",
            " 37%|███▋      | 152401920/407873900 [00:02<00:03, 70881307.16B/s]\u001b[A\n",
            " 39%|███▉      | 159745024/407873900 [00:02<00:03, 69382051.12B/s]\u001b[A\n",
            " 41%|████      | 166868992/407873900 [00:02<00:03, 68932556.77B/s]\u001b[A\n",
            " 43%|████▎     | 174264320/407873900 [00:02<00:03, 70364335.52B/s]\u001b[A\n",
            " 45%|████▍     | 182144000/407873900 [00:02<00:03, 72697823.96B/s]\u001b[A\n",
            " 47%|████▋     | 190117888/407873900 [00:02<00:02, 74674492.93B/s]\u001b[A\n",
            " 49%|████▊     | 198269952/407873900 [00:02<00:02, 76604396.11B/s]\u001b[A\n",
            " 51%|█████     | 206582784/407873900 [00:02<00:02, 78451005.67B/s]\u001b[A\n",
            " 53%|█████▎    | 214484992/407873900 [00:03<00:02, 78397930.05B/s]\u001b[A\n",
            " 55%|█████▍    | 222689280/407873900 [00:03<00:02, 79451467.52B/s]\u001b[A\n",
            " 57%|█████▋    | 231165952/407873900 [00:03<00:02, 80973681.41B/s]\u001b[A\n",
            " 59%|█████▊    | 239310848/407873900 [00:03<00:02, 81112122.40B/s]\u001b[A\n",
            " 61%|██████    | 247442432/407873900 [00:03<00:02, 78161079.03B/s]\u001b[A\n",
            " 63%|██████▎   | 255315968/407873900 [00:03<00:01, 78332058.13B/s]\u001b[A\n",
            " 65%|██████▍   | 263791616/407873900 [00:03<00:01, 80153300.96B/s]\u001b[A\n",
            " 67%|██████▋   | 271836160/407873900 [00:03<00:01, 69448633.02B/s]\u001b[A\n",
            " 68%|██████▊   | 279056384/407873900 [00:03<00:01, 70083327.37B/s]\u001b[A\n",
            " 70%|███████   | 286921728/407873900 [00:03<00:01, 72448647.68B/s]\u001b[A\n",
            " 72%|███████▏  | 294323200/407873900 [00:04<00:01, 71441240.80B/s]\u001b[A\n",
            " 74%|███████▍  | 301864960/407873900 [00:04<00:01, 72581739.07B/s]\u001b[A\n",
            " 76%|███████▌  | 309587968/407873900 [00:04<00:01, 73915163.39B/s]\u001b[A\n",
            " 78%|███████▊  | 317220864/407873900 [00:04<00:01, 74620791.69B/s]\u001b[A\n",
            " 80%|███████▉  | 325198848/407873900 [00:04<00:01, 76096925.95B/s]\u001b[A\n",
            " 82%|████████▏ | 333159424/407873900 [00:04<00:00, 77114973.64B/s]\u001b[A\n",
            " 84%|████████▎ | 340902912/407873900 [00:04<00:00, 76946620.75B/s]\u001b[A\n",
            " 85%|████████▌ | 348619776/407873900 [00:04<00:00, 76123545.73B/s]\u001b[A\n",
            " 87%|████████▋ | 356249600/407873900 [00:04<00:00, 76057759.82B/s]\u001b[A\n",
            " 89%|████████▉ | 364438528/407873900 [00:04<00:00, 77717333.99B/s]\u001b[A\n",
            " 91%|█████████▏| 372228096/407873900 [00:05<00:00, 76895969.68B/s]\u001b[A\n",
            " 93%|█████████▎| 379932672/407873900 [00:05<00:00, 75898996.67B/s]\u001b[A\n",
            " 95%|█████████▌| 387997696/407873900 [00:05<00:00, 77263441.69B/s]\u001b[A\n",
            " 97%|█████████▋| 396519424/407873900 [00:05<00:00, 79488587.28B/s]\u001b[A\n",
            " 99%|█████████▉| 404554752/407873900 [00:05<00:00, 79743793.10B/s]\u001b[A\n",
            "100%|██████████| 407873900/407873900 [00:05<00:00, 73705173.82B/s]\u001b[A01/23/2020 08:07:35 - INFO - pytorch_pretrained_bert.file_utils -   copying /tmp/tmpsi91l5g7 to cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "01/23/2020 08:07:36 - INFO - pytorch_pretrained_bert.file_utils -   creating metadata file for /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "01/23/2020 08:07:36 - INFO - pytorch_pretrained_bert.file_utils -   removing temp file /tmp/tmpsi91l5g7\n",
            "01/23/2020 08:07:36 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "01/23/2020 08:07:36 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpc53r7zc8\n",
            "01/23/2020 08:07:41 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPYc6qiFit8b",
        "colab_type": "text"
      },
      "source": [
        "###Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QD9HwPe3DDv",
        "colab_type": "code",
        "outputId": "875ceec7-5579-4e53-a138-21c56590613d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# training\n",
        "logger.info(\"***** Running training *****\")\n",
        "logger.info(\"  Num examples = %d\", len(train_examples))\n",
        "logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
        "logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
        "model.train()\n",
        "global_step = 0\n",
        "for i in range(int(args.num_train_epochs)):\n",
        "    # do training\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Train iter\", position=0)):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        logits = model(*batch)\n",
        "        loss, _ = cal_performance(logits, batch[2])\n",
        "\n",
        "        if args.gradient_accumulation_steps > 1:\n",
        "            loss = loss / args.gradient_accumulation_steps\n",
        "        loss.backward()\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += batch[0].size(0)\n",
        "        nb_tr_steps += 1\n",
        "        if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "        if (step + 1) % args.print_every == 0:\n",
        "            logger.info(f'Epoch {i}, step {step}, loss {loss.item()}.')\n",
        "            logger.info(f'Ground: {\"\".join(tokenizer.convert_ids_to_tokens(batch[2][0].cpu().numpy()))}')\n",
        "            logger.info(f'Generated: {\"\".join(tokenizer.convert_ids_to_tokens(logits[0].max(-1)[1].cpu().numpy()))}')\n",
        "    \n",
        "    #do save model\n",
        "    if args.output_dir is not None:\n",
        "        state_dict = model.state_dict()\n",
        "        torch.save(state_dict, os.path.join(model_path, 'BertAbsSum_{}.bin'.format(i)))\n",
        "        logger.info('Model saved')\n",
        "    \n",
        "    # do evaluation\n",
        "    if valid_dataloader is not None:\n",
        "        model.eval()\n",
        "        batch = next(iter(valid_dataloader))\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # beam_decode\n",
        "        pred, _ = model.beam_decode(batch[0], batch[1], 3, 3)\n",
        "        # pred = model.greedy_decode(batch[0], batch[1])\n",
        "        logger.info(f'Source: {\"\".join(tokenizer.convert_ids_to_tokens(batch[0][0].cpu().numpy()))}')\n",
        "        logger.info(f'Beam Generated: {\"\".join(tokenizer.convert_ids_to_tokens(pred[0][0]))}')\n",
        "        # logger.info(f'Beam Generated: {tokenizer.convert_ids_to_tokens(pred[0].cpu().numpy())}')\n",
        "    \n",
        "    # do validate        \n",
        "    do_validate()\n",
        "\n",
        "    logger.info(f'Epoch {i} finished.')\n",
        "with open(os.path.join(args.bert_model, 'bert_config.json'), 'r') as f:\n",
        "    bert_config = json.load(f)\n",
        "config = {'bert_config': bert_config, 'decoder_config': decoder_config}\n",
        "with open(os.path.join(model_path, 'config.json'), 'w') as f:\n",
        "    json.dump(config, f)\n",
        "logger.info('Training finished')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01/23/2020 08:08:01 - INFO - __main__ -   ***** Running training *****\n",
            "01/23/2020 08:08:01 - INFO - __main__ -     Num examples = 121489\n",
            "01/23/2020 08:08:01 - INFO - __main__ -     Batch size = 32\n",
            "01/23/2020 08:08:01 - INFO - __main__ -     Num steps = 36444\n",
            "Train iter:   3%|▎         | 99/3796 [01:53<1:14:08,  1.20s/it]01/23/2020 08:09:56 - INFO - __main__ -   Epoch 0, step 99, loss 7.659669876098633.\n",
            "01/23/2020 08:09:56 - INFO - __main__ -   Ground: [CLS]quan##ti##zationofaf##finebodies.theoryandapplicationsinmechanicsofstructuredmedia[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:09:56 - INFO - __main__ -   Generated: [SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]detectiondetectiondetectiondetectiondetectiondetectiondetectiondetectiondetectiondetectiondetection\n",
            "Train iter:   5%|▌         | 199/3796 [03:55<1:13:38,  1.23s/it]01/23/2020 08:11:58 - INFO - __main__ -   Epoch 0, step 199, loss 7.728640556335449.\n",
            "01/23/2020 08:11:58 - INFO - __main__ -   Ground: [CLS]optimalconstructionofk-nearestneighborgraphsforidentifyingnoisyclusters[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:11:58 - INFO - __main__ -   Generated: [SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]detectiondetectiondetectiondetectiondetectiondetectiondetectiondetectiondetectiondetectiondetectiondetectiondetectiondetectiondetection\n",
            "Train iter:   8%|▊         | 299/3796 [05:58<1:11:25,  1.23s/it]01/23/2020 08:14:01 - INFO - __main__ -   Epoch 0, step 299, loss 7.752091407775879.\n",
            "01/23/2020 08:14:01 - INFO - __main__ -   Ground: [CLS]nonlinearfactormodelsfornetworkandpaneldata[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:14:01 - INFO - __main__ -   Generated: [SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]networknetworknetworknetworknetworknetworknetworknetworknetworknetworknetworknetworknetworknetworknetworknetworknetworknetworknetwork\n",
            "Train iter:  11%|█         | 399/3796 [08:00<1:09:16,  1.22s/it]01/23/2020 08:16:03 - INFO - __main__ -   Epoch 0, step 399, loss 7.683202743530273.\n",
            "01/23/2020 08:16:03 - INFO - __main__ -   Ground: [CLS]supportvectormachineforfunctionaldataclassification[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:16:03 - INFO - __main__ -   Generated: [SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]riskriskriskriskriskriskriskriskriskriskriskriskriskriskriskriskriskriskriskrisk\n",
            "Train iter:  13%|█▎        | 499/3796 [10:03<1:06:52,  1.22s/it]01/23/2020 08:18:06 - INFO - __main__ -   Epoch 0, step 499, loss 7.546125411987305.\n",
            "01/23/2020 08:18:06 - INFO - __main__ -   Ground: [CLS]optionalp\\'{o}l##yatreeandbay##esianinference[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:18:06 - INFO - __main__ -   Generated: [SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-------------\n",
            "Train iter:  16%|█▌        | 599/3796 [12:05<1:05:38,  1.23s/it]01/23/2020 08:20:08 - INFO - __main__ -   Epoch 0, step 599, loss 7.734494686126709.\n",
            "01/23/2020 08:20:08 - INFO - __main__ -   Ground: [CLS]fourpapersoncontemporarysoftwaredesignstrategiesforstatisticalmethod##ologists[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:20:08 - INFO - __main__ -   Generated: [SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]----------------\n",
            "Train iter:  18%|█▊        | 699/3796 [14:08<1:03:11,  1.22s/it]01/23/2020 08:22:11 - INFO - __main__ -   Epoch 0, step 699, loss 7.82793664932251.\n",
            "01/23/2020 08:22:11 - INFO - __main__ -   Ground: [CLS]ag##nosticsystemidentificationformodel-basedreinforcementlearning[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:22:11 - INFO - __main__ -   Generated: [SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-----------------\n",
            "Train iter:  21%|██        | 799/3796 [16:11<1:01:25,  1.23s/it]01/23/2020 08:24:14 - INFO - __main__ -   Epoch 0, step 799, loss 7.550712585449219.\n",
            "01/23/2020 08:24:14 - INFO - __main__ -   Ground: [CLS]insearchoftheblackswan:analysisofthestatisticalevidenceofelectoralfraudinvenezuela[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:24:14 - INFO - __main__ -   Generated: [SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]----------\n",
            "Train iter:  24%|██▎       | 899/3796 [18:13<59:02,  1.22s/it]01/23/2020 08:26:16 - INFO - __main__ -   Epoch 0, step 899, loss 7.594971656799316.\n",
            "01/23/2020 08:26:16 - INFO - __main__ -   Ground: [CLS]generalizedboost##ingalgorithmsforconvexoptimization[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:26:16 - INFO - __main__ -   Generated: [SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]--------------------\n",
            "Train iter:  26%|██▋       | 999/3796 [20:15<56:52,  1.22s/it]01/23/2020 08:28:18 - INFO - __main__ -   Epoch 0, step 999, loss 7.5051703453063965.\n",
            "01/23/2020 08:28:18 - INFO - __main__ -   Ground: [CLS]discoveringeffectmodificationinanobservation##alstudyofsurgicalmortalityathospitalswithsuperiornursing[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:28:18 - INFO - __main__ -   Generated: [SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-----------\n",
            "Train iter:  29%|██▉       | 1099/3796 [22:18<55:07,  1.23s/it]01/23/2020 08:30:21 - INFO - __main__ -   Epoch 0, step 1099, loss 7.6546454429626465.\n",
            "01/23/2020 08:30:21 - INFO - __main__ -   Ground: [CLS]equivalentcircuitprogrammingforest##imatingthestateofapowersystem[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:30:21 - INFO - __main__ -   Generated: [SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]---------------\n",
            "Train iter:  32%|███▏      | 1199/3796 [24:20<52:59,  1.22s/it]01/23/2020 08:32:23 - INFO - __main__ -   Epoch 0, step 1199, loss 7.615110874176025.\n",
            "01/23/2020 08:32:23 - INFO - __main__ -   Ground: [CLS]positivedefinite$\\el##l_1$penal##izedestimationoflargeco##var##iancematrices[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:32:23 - INFO - __main__ -   Generated: [SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]---------\n",
            "Train iter:  34%|███▍      | 1299/3796 [26:23<50:52,  1.22s/it]01/23/2020 08:34:26 - INFO - __main__ -   Epoch 0, step 1299, loss 7.5869364738464355.\n",
            "01/23/2020 08:34:26 - INFO - __main__ -   Ground: [CLS]statepricedensityestimationvianon##para##metricmixture##s[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:34:26 - INFO - __main__ -   Generated: [SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-----------------\n",
            "Train iter:  37%|███▋      | 1399/3796 [28:25<49:02,  1.23s/it]01/23/2020 08:36:28 - INFO - __main__ -   Epoch 0, step 1399, loss 7.770173072814941.\n",
            "01/23/2020 08:36:28 - INFO - __main__ -   Ground: [CLS]dynamicup##link/down##linkresourcemanagementinflexibledu##plex-enabledwirelessnetworks[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:36:28 - INFO - __main__ -   Generated: [SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-----------\n",
            "Train iter:  39%|███▉      | 1499/3796 [30:27<46:42,  1.22s/it]01/23/2020 08:38:30 - INFO - __main__ -   Epoch 0, step 1499, loss 7.510799407958984.\n",
            "01/23/2020 08:38:30 - INFO - __main__ -   Ground: [CLS]safeandnear-optimalpolicylearningformodelpredict##ivecontrolusingprimal-dualneuralnetworks[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:38:30 - INFO - __main__ -   Generated: of[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]---------\n",
            "Train iter:  42%|████▏     | 1599/3796 [32:29<44:49,  1.22s/it]01/23/2020 08:40:32 - INFO - __main__ -   Epoch 0, step 1599, loss 7.585951328277588.\n",
            "01/23/2020 08:40:32 - INFO - __main__ -   Ground: [CLS]verticesfromreplicainarandommatrixtheory[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:40:32 - INFO - __main__ -   Generated: [SEP]--[SEP]---[SEP]---------------------\n",
            "Train iter:  45%|████▍     | 1699/3796 [34:32<42:55,  1.23s/it]01/23/2020 08:42:35 - INFO - __main__ -   Epoch 0, step 1699, loss 7.6447672843933105.\n",
            "01/23/2020 08:42:35 - INFO - __main__ -   Ground: [CLS]coherenthardx-raysfromat##tose##con##dpulsetrain-assistedharmonicgeneration[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:42:35 - INFO - __main__ -   Generated: [SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-----------\n",
            "Train iter:  47%|████▋     | 1799/3796 [36:34<40:37,  1.22s/it]01/23/2020 08:44:37 - INFO - __main__ -   Epoch 0, step 1799, loss 7.457595348358154.\n",
            "01/23/2020 08:44:37 - INFO - __main__ -   Ground: [CLS]asimulatedanne##alingapproachtobay##esianinference[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:44:37 - INFO - __main__ -   Generated: of[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]------------------\n",
            "Train iter:  50%|█████     | 1899/3796 [38:36<38:35,  1.22s/it]01/23/2020 08:46:39 - INFO - __main__ -   Epoch 0, step 1899, loss 7.551198482513428.\n",
            "01/23/2020 08:46:39 - INFO - __main__ -   Ground: [CLS]neuralnetwork-basedcluster##ingusingpair##wiseconstraints[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:46:39 - INFO - __main__ -   Generated: ofofofof[SEP]ofofof[SEP][SEP][SEP][SEP]-----------------\n",
            "Train iter:  53%|█████▎    | 1999/3796 [40:38<36:32,  1.22s/it]01/23/2020 08:48:41 - INFO - __main__ -   Epoch 0, step 1999, loss 7.570465564727783.\n",
            "01/23/2020 08:48:41 - INFO - __main__ -   Ground: [CLS]themetalabundanceofci##rc##um##nu##cle##arstarformingregionsinearlytypespiral##s.spec##tro##ph##oto##metricobservations[SEP][PAD][PAD][PAD]\n",
            "01/23/2020 08:48:41 - INFO - __main__ -   Generated: of[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]--\n",
            "Train iter:  55%|█████▌    | 2099/3796 [42:40<34:32,  1.22s/it]01/23/2020 08:50:43 - INFO - __main__ -   Epoch 0, step 2099, loss 7.554435729980469.\n",
            "01/23/2020 08:50:43 - INFO - __main__ -   Ground: [CLS]atutor##ialonprincipalcomponentanalysiswiththeaccord.netframework[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:50:43 - INFO - __main__ -   Generated: aof-ofof-[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]--------------\n",
            "Train iter:  58%|█████▊    | 2199/3796 [44:42<32:19,  1.21s/it]01/23/2020 08:52:45 - INFO - __main__ -   Epoch 0, step 2199, loss 7.665243625640869.\n",
            "01/23/2020 08:52:45 - INFO - __main__ -   Ground: [CLS]discussionof:statisticalanalysisofanarcheologicalfind[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:52:45 - INFO - __main__ -   Generated: ofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]------------------\n",
            "Train iter:  61%|██████    | 2299/3796 [46:43<30:15,  1.21s/it]01/23/2020 08:54:46 - INFO - __main__ -   Epoch 0, step 2299, loss 7.587794780731201.\n",
            "01/23/2020 08:54:46 - INFO - __main__ -   Ground: [CLS]optimalbay##esianestimationinst##och##asticblockmodels[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:54:46 - INFO - __main__ -   Generated: ofof[SEP][SEP]ofof[SEP][SEP][SEP][SEP][SEP][SEP]-----------------\n",
            "Train iter:  63%|██████▎   | 2399/3796 [48:44<27:59,  1.20s/it]01/23/2020 08:56:46 - INFO - __main__ -   Epoch 0, step 2399, loss 7.596853733062744.\n",
            "01/23/2020 08:56:46 - INFO - __main__ -   Ground: [CLS]dynamicpowerallocationanduserschedulingforpower-efficientandlow-late##ncycommunications[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:56:47 - INFO - __main__ -   Generated: ofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-----------\n",
            "Train iter:  66%|██████▌   | 2499/3796 [50:44<26:04,  1.21s/it]01/23/2020 08:58:47 - INFO - __main__ -   Epoch 0, step 2499, loss 7.494767665863037.\n",
            "01/23/2020 08:58:47 - INFO - __main__ -   Ground: [CLS]thewindowattheedgeofchaosinasimplemodelofgeneinteractionnetworks[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 08:58:47 - INFO - __main__ -   Generated: aaaofof[SEP]of[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]------------\n",
            "Train iter:  68%|██████▊   | 2599/3796 [52:45<24:01,  1.20s/it]01/23/2020 09:00:48 - INFO - __main__ -   Epoch 0, step 2599, loss 7.535849571228027.\n",
            "01/23/2020 09:00:48 - INFO - __main__ -   Ground: [CLS]bay##esianinferenceforaco##var##iancematrix[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 09:00:48 - INFO - __main__ -   Generated: aofofofof[SEP]ofof[SEP][SEP]of------------------\n",
            "Train iter:  71%|███████   | 2699/3796 [54:46<22:07,  1.21s/it]01/23/2020 09:02:49 - INFO - __main__ -   Epoch 0, step 2699, loss 7.5485944747924805.\n",
            "01/23/2020 09:02:49 - INFO - __main__ -   Ground: [CLS]aninformation-basedtrafficcontrolinapublicconvey##ancesystem:reducedcluster##ingandenhancedefficiency[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 09:02:49 - INFO - __main__ -   Generated: a[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]--------\n",
            "Train iter:  74%|███████▎  | 2799/3796 [56:46<19:57,  1.20s/it]01/23/2020 09:04:49 - INFO - __main__ -   Epoch 0, step 2799, loss 7.709637641906738.\n",
            "01/23/2020 09:04:49 - INFO - __main__ -   Ground: [CLS]thelikelihoodprincipledoesnoten##taila`surething',`evildemon'or`deter##mini##st'hypothesis[SEP][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 09:04:49 - INFO - __main__ -   Generated: aof[SEP]ofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]---\n",
            "Train iter:  76%|███████▋  | 2899/3796 [58:46<17:55,  1.20s/it]01/23/2020 09:06:49 - INFO - __main__ -   Epoch 0, step 2899, loss 7.688543319702148.\n",
            "01/23/2020 09:06:49 - INFO - __main__ -   Ground: [CLS]confidenceintervalsformaxim##ineffectsinin##hom##ogen##eouslarge-scaledata[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 09:06:49 - INFO - __main__ -   Generated: ofaoftheofofofofofofof[SEP][SEP][SEP][SEP][SEP][SEP]------------\n",
            "Train iter:  79%|███████▉  | 2999/3796 [1:00:47<16:01,  1.21s/it]01/23/2020 09:08:50 - INFO - __main__ -   Epoch 0, step 2999, loss 7.351975917816162.\n",
            "01/23/2020 09:08:50 - INFO - __main__ -   Ground: [CLS]statisticalinferenceofacanonicaldictionaryofproteinsub##st##ru##ct##uralfragments[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 09:08:50 - INFO - __main__ -   Generated: a[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-------------\n",
            "Train iter:  82%|████████▏ | 3099/3796 [1:02:48<14:02,  1.21s/it]01/23/2020 09:10:51 - INFO - __main__ -   Epoch 0, step 3099, loss 7.564845561981201.\n",
            "01/23/2020 09:10:51 - INFO - __main__ -   Ground: [CLS]mutationclustersfromcancerex##ome[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 09:10:51 - INFO - __main__ -   Generated: theofofofofofofof---------------------\n",
            "Train iter:  84%|████████▍ | 3199/3796 [1:04:48<11:55,  1.20s/it]01/23/2020 09:12:51 - INFO - __main__ -   Epoch 0, step 3199, loss 7.4237380027771.\n",
            "01/23/2020 09:12:51 - INFO - __main__ -   Ground: [CLS]compoundrealwish##artandq-wish##artmatrices[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 09:12:51 - INFO - __main__ -   Generated: theof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-----------------\n",
            "Train iter:  87%|████████▋ | 3299/3796 [1:06:47<09:53,  1.19s/it]01/23/2020 09:14:50 - INFO - __main__ -   Epoch 0, step 3299, loss 7.7043375968933105.\n",
            "01/23/2020 09:14:50 - INFO - __main__ -   Ground: [CLS]forceddes##or##ptionofsemi##fle##xi##blepolymers,ads##or##bedanddrivenbymolecularmotors[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 09:14:50 - INFO - __main__ -   Generated: aofofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]--------\n",
            "Train iter:  90%|████████▉ | 3399/3796 [1:08:47<07:56,  1.20s/it]01/23/2020 09:16:50 - INFO - __main__ -   Epoch 0, step 3399, loss 7.584712505340576.\n",
            "01/23/2020 09:16:50 - INFO - __main__ -   Ground: [CLS]ashort-loopalgorithmforquantummontecarlosimulations[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 09:16:50 - INFO - __main__ -   Generated: atheofofofof[SEP][SEP][SEP][SEP][SEP][SEP]-----------------\n",
            "Train iter:  92%|█████████▏| 3499/3796 [1:10:47<05:54,  1.19s/it]01/23/2020 09:18:50 - INFO - __main__ -   Epoch 0, step 3499, loss 7.609558582305908.\n",
            "01/23/2020 09:18:50 - INFO - __main__ -   Ground: [CLS]theburdenofhivinapublichospitalinjohannesburg,southafrica[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 09:18:50 - INFO - __main__ -   Generated: aof[SEP][SEP][SEP][SEP]of[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]--------------\n",
            "Train iter:  95%|█████████▍| 3599/3796 [1:12:46<03:55,  1.19s/it]01/23/2020 09:20:49 - INFO - __main__ -   Epoch 0, step 3599, loss 7.544293403625488.\n",
            "01/23/2020 09:20:49 - INFO - __main__ -   Ground: [CLS]ontheun##ip##ote##ntcharactersofthere##egroupsoftypeg_2[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 09:20:49 - INFO - __main__ -   Generated: aof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]----------\n",
            "Train iter:  97%|█████████▋| 3699/3796 [1:14:46<01:55,  1.19s/it]01/23/2020 09:22:49 - INFO - __main__ -   Epoch 0, step 3699, loss 7.394804954528809.\n",
            "01/23/2020 09:22:49 - INFO - __main__ -   Ground: [CLS]distributedlearninginnon-convexenvironments--partii:polynomialescapefromsaddle-points[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 09:22:49 - INFO - __main__ -   Generated: a-----[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]---------\n",
            "Train iter: 100%|██████████| 3796/3796 [1:16:43<00:00,  1.20s/it]\n",
            "01/23/2020 09:24:47 - INFO - __main__ -   Model saved\n",
            "01/23/2020 09:24:56 - INFO - __main__ -   Source: [CLS]theoutputscoresofaneuralnetworkclass##ifierareconvertedtopro##ba##bilitiesvianormal##izingoverthescoresofallcompetingcategories.computingthispartitionfunction,$z$,isthenlinearinthenumberofcategories,whichisproblematicasreal-worldproblemsetscontinuetogrowincat##egoricaltypes,suchasinvisualobjectrecognitionordisc##rim##ina##tivelanguagemodeling.weproposethreeapproachesforsub##line##arestimationofthepartitionfunction,basedonapproximatenearestneighborsearchandkernelfeaturemapsandcomparetheperformanceoftheproposedapproachesempirical##ly.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 09:24:56 - INFO - __main__ -   Beam Generated: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
            "01/23/2020 09:24:56 - INFO - __main__ -   ***** Running validation *****\n",
            "Val iter: 100%|██████████| 422/422 [56:10<00:00,  7.89s/it]\n",
            "01/23/2020 10:21:07 - INFO - __main__ -   ******Validation results******\n",
            "01/23/2020 10:21:07 - INFO - __main__ -   Rouge-1: 0.0\n",
            "01/23/2020 10:21:07 - INFO - __main__ -   Rouge-2: 0.0\n",
            "01/23/2020 10:21:07 - INFO - __main__ -   Validation finished.\n",
            "01/23/2020 10:21:07 - INFO - __main__ -   Epoch 0 finished.\n",
            "Train iter:   3%|▎         | 99/3796 [01:58<1:13:56,  1.20s/it]01/23/2020 10:23:07 - INFO - __main__ -   Epoch 1, step 99, loss 7.5876264572143555.\n",
            "01/23/2020 10:23:07 - INFO - __main__ -   Ground: [CLS]as##ym##pt##otic##allyefficientestimationofascaleparameteringa##uss##iantimeseriesandclosed-formexpressionsforthefisherinformation[SEP][PAD][PAD]\n",
            "01/23/2020 10:23:07 - INFO - __main__ -   Generated: aofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-\n",
            "Train iter:   5%|▌         | 199/3796 [03:59<1:11:55,  1.20s/it]01/23/2020 10:25:07 - INFO - __main__ -   Epoch 1, step 199, loss 7.472227096557617.\n",
            "01/23/2020 10:25:07 - INFO - __main__ -   Ground: [CLS]onmetric##sofpositiveric##cicurvatureconform##altomx##r^m[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 10:25:07 - INFO - __main__ -   Generated: theofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]------------\n",
            "Train iter:   8%|▊         | 299/3796 [05:59<1:10:01,  1.20s/it]01/23/2020 10:27:07 - INFO - __main__ -   Epoch 1, step 299, loss 7.703167915344238.\n",
            "01/23/2020 10:27:07 - INFO - __main__ -   Ground: [CLS]graph##ene:apseudo##chi##ralfe##rmiliquid[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 10:27:07 - INFO - __main__ -   Generated: ofofofofofof---[SEP][SEP][SEP]-----------------\n",
            "Train iter:  11%|█         | 399/3796 [07:59<1:08:00,  1.20s/it]01/23/2020 10:29:07 - INFO - __main__ -   Epoch 1, step 399, loss 7.605070114135742.\n",
            "01/23/2020 10:29:07 - INFO - __main__ -   Ground: [CLS]measuringmulti##var##iateredundantinformationwithpoint##wisecommonchangeinsur##pr##isa##l[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 10:29:07 - INFO - __main__ -   Generated: aofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-----------\n",
            "Train iter:  13%|█▎        | 499/3796 [09:59<1:05:18,  1.19s/it]01/23/2020 10:31:07 - INFO - __main__ -   Epoch 1, step 499, loss 7.464786529541016.\n",
            "01/23/2020 10:31:07 - INFO - __main__ -   Ground: [CLS]sequentialtestsofmultipleh##yp##oth##esescontrollingtypeiandiifamily##wiseerrorrates[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 10:31:07 - INFO - __main__ -   Generated: aofof-ofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]----------\n",
            "Train iter:  16%|█▌        | 599/3796 [11:58<1:03:38,  1.19s/it]01/23/2020 10:33:07 - INFO - __main__ -   Epoch 1, step 599, loss 7.426501274108887.\n",
            "01/23/2020 10:33:07 - INFO - __main__ -   Ground: [CLS]topologicalandnetworkanalysisoflithiumionbatterycomponents:theimportanceofpor##espaceconnectivityforcelloperation[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 10:33:07 - INFO - __main__ -   Generated: atheofofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-------\n",
            "Train iter:  18%|█▊        | 699/3796 [13:58<1:01:34,  1.19s/it]01/23/2020 10:35:06 - INFO - __main__ -   Epoch 1, step 699, loss 7.510477542877197.\n",
            "01/23/2020 10:35:06 - INFO - __main__ -   Ground: [CLS]sk##ew##edtargetrangestrategyformulti##per##io##dportfoliooptimizationusingatwo-stageleastsquaresmontecarlomethod[SEP][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 10:35:06 - INFO - __main__ -   Generated: aofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]----\n",
            "Train iter:  21%|██        | 799/3796 [15:57<59:44,  1.20s/it]01/23/2020 10:37:06 - INFO - __main__ -   Epoch 1, step 799, loss 7.601728439331055.\n",
            "01/23/2020 10:37:06 - INFO - __main__ -   Ground: [CLS]testingpara##metricmodelsinlinear-directionalregression[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 10:37:06 - INFO - __main__ -   Generated: aofofof[SEP]of[SEP]of[SEP][SEP][SEP]------------------\n",
            "Train iter:  24%|██▎       | 899/3796 [17:57<57:35,  1.19s/it]01/23/2020 10:39:05 - INFO - __main__ -   Epoch 1, step 899, loss 7.508730888366699.\n",
            "01/23/2020 10:39:05 - INFO - __main__ -   Ground: [CLS]universalman##del##bro##tsetasamodelofphasetransitiontheory[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 10:39:05 - INFO - __main__ -   Generated: athetheof-ofofofof-[SEP][SEP]-of---------------\n",
            "Train iter:  26%|██▋       | 999/3796 [19:56<55:42,  1.20s/it]01/23/2020 10:41:05 - INFO - __main__ -   Epoch 1, step 999, loss 7.437270641326904.\n",
            "01/23/2020 10:41:05 - INFO - __main__ -   Ground: [CLS]selectionfromastablebox[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 10:41:05 - INFO - __main__ -   Generated: aofofof[SEP][SEP][SEP]----------------------\n",
            "Train iter:  29%|██▉       | 1099/3796 [21:56<53:50,  1.20s/it]01/23/2020 10:43:04 - INFO - __main__ -   Epoch 1, step 1099, loss 7.628826141357422.\n",
            "01/23/2020 10:43:04 - INFO - __main__ -   Ground: [CLS]variation##alinferenceforon-lineanomalydetectioninhigh-dimensionaltimeseries[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 10:43:04 - INFO - __main__ -   Generated: atheofofof[SEP]-of[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]------------\n",
            "Train iter:  32%|███▏      | 1199/3796 [23:55<51:35,  1.19s/it]01/23/2020 10:45:03 - INFO - __main__ -   Epoch 1, step 1199, loss 7.596151828765869.\n",
            "01/23/2020 10:45:04 - INFO - __main__ -   Ground: [CLS]anoptimalconsumption-investmentmodelwithconstraintonconsumption[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 10:45:04 - INFO - __main__ -   Generated: aofofofofofof[SEP][SEP][SEP][SEP][SEP]-----------------\n",
            "Train iter:  34%|███▍      | 1299/3796 [25:55<49:40,  1.19s/it]01/23/2020 10:47:03 - INFO - __main__ -   Epoch 1, step 1299, loss 7.398355007171631.\n",
            "01/23/2020 10:47:03 - INFO - __main__ -   Ground: [CLS]semi-instrumentalvariables:atestforinstrumentad##mis##sibility[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 10:47:03 - INFO - __main__ -   Generated: atheof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]---------------\n",
            "Train iter:  37%|███▋      | 1399/3796 [27:54<47:52,  1.20s/it]01/23/2020 10:49:03 - INFO - __main__ -   Epoch 1, step 1399, loss 7.462709426879883.\n",
            "01/23/2020 10:49:03 - INFO - __main__ -   Ground: [CLS]dynamicstateestimationbasedonpo##issonspiketrains:towardsatheoryofoptimalencoding[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 10:49:03 - INFO - __main__ -   Generated: aofofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-----------\n",
            "Train iter:  39%|███▉      | 1499/3796 [29:54<45:41,  1.19s/it]01/23/2020 10:51:02 - INFO - __main__ -   Epoch 1, step 1499, loss 7.62286901473999.\n",
            "01/23/2020 10:51:02 - INFO - __main__ -   Ground: [CLS]anadaptive##lyweightedstat##isticfordetectingdifferentialgeneexpressionwhencombiningmultipletranscript##omicstudies[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 10:51:02 - INFO - __main__ -   Generated: theofofofofofofof[SEP]ofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]----------\n",
            "Train iter:  42%|████▏     | 1599/3796 [31:53<44:00,  1.20s/it]01/23/2020 10:53:02 - INFO - __main__ -   Epoch 1, step 1599, loss 7.526175022125244.\n",
            "01/23/2020 10:53:02 - INFO - __main__ -   Ground: [CLS]segment##ationofthemeanofhet##eros##ced##asticdataviacross-validation[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 10:53:02 - INFO - __main__ -   Generated: theofofofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]------------\n",
            "Train iter:  45%|████▍     | 1699/3796 [33:53<41:46,  1.20s/it]01/23/2020 10:55:01 - INFO - __main__ -   Epoch 1, step 1699, loss 7.4456071853637695.\n",
            "01/23/2020 10:55:01 - INFO - __main__ -   Ground: [CLS]sequentialbay##esianinferenceinhiddenmarko##vst##och##astickineticmodelswithapplicationtodetectionandresponsetoseasonalepidemic##s[SEP][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 10:55:01 - INFO - __main__ -   Generated: aaofof[SEP]ofofofofofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]----\n",
            "Train iter:  47%|████▋     | 1799/3796 [35:52<39:36,  1.19s/it]01/23/2020 10:57:01 - INFO - __main__ -   Epoch 1, step 1799, loss 7.540304183959961.\n",
            "01/23/2020 10:57:01 - INFO - __main__ -   Ground: [CLS]matrixcompletionandperformanceguaranteesforsingleindividualha##pl##ot##yp##ing[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 10:57:01 - INFO - __main__ -   Generated: theofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]--------------\n",
            "Train iter:  50%|█████     | 1899/3796 [37:52<37:47,  1.20s/it]01/23/2020 10:59:00 - INFO - __main__ -   Epoch 1, step 1899, loss 7.739540100097656.\n",
            "01/23/2020 10:59:00 - INFO - __main__ -   Ground: [CLS]onvo##rti##cesandsol##ito##nsingold##stoneandabel##ian-hi##ggsmodels[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 10:59:00 - INFO - __main__ -   Generated: aofofofofofofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]---------\n",
            "Train iter:  53%|█████▎    | 1999/3796 [39:51<35:42,  1.19s/it]01/23/2020 11:01:00 - INFO - __main__ -   Epoch 1, step 1999, loss 7.451849460601807.\n",
            "01/23/2020 11:01:00 - INFO - __main__ -   Ground: [CLS]proportionestimationbasedonapartiallyrankorderedsetsamplewithmultiplecon##com##itan##tsinabreastcancerstudy[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 11:01:00 - INFO - __main__ -   Generated: aofofofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]------\n",
            "Train iter:  55%|█████▌    | 2099/3796 [41:51<33:46,  1.19s/it]01/23/2020 11:02:59 - INFO - __main__ -   Epoch 1, step 2099, loss 7.439188480377197.\n",
            "01/23/2020 11:02:59 - INFO - __main__ -   Ground: [CLS]consistentestimationofdynamicandmulti-layerblockmodels[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 11:02:59 - INFO - __main__ -   Generated: thetheofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP]-----------------\n",
            "Train iter:  58%|█████▊    | 2199/3796 [43:50<31:42,  1.19s/it]01/23/2020 11:04:59 - INFO - __main__ -   Epoch 1, step 2199, loss 7.529344081878662.\n",
            "01/23/2020 11:04:59 - INFO - __main__ -   Ground: [CLS]aframeworkofcon##ju##gatedirectionmethodsforsymmetriclinearsystemsinoptimization[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 11:04:59 - INFO - __main__ -   Generated: aofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-------------\n",
            "Train iter:  61%|██████    | 2299/3796 [45:50<29:50,  1.20s/it]01/23/2020 11:06:58 - INFO - __main__ -   Epoch 1, step 2299, loss 7.52119255065918.\n",
            "01/23/2020 11:06:58 - INFO - __main__ -   Ground: [CLS]theassessmentofthenearinfraredidentificationofcarbonstars.i.thelocalgroupgalaxiesw##lm,ic10andngc68##22[SEP][PAD][PAD]\n",
            "01/23/2020 11:06:58 - INFO - __main__ -   Generated: theofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-\n",
            "Train iter:  63%|██████▎   | 2399/3796 [47:49<27:43,  1.19s/it]01/23/2020 11:08:57 - INFO - __main__ -   Epoch 1, step 2399, loss 7.601357936859131.\n",
            "01/23/2020 11:08:57 - INFO - __main__ -   Ground: [CLS]regular##izedemalgorithms:aunifiedframeworkandstatisticalguarantees[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 11:08:57 - INFO - __main__ -   Generated: aa-the----[SEP][SEP][SEP][SEP][SEP]----------------\n",
            "Train iter:  66%|██████▌   | 2499/3796 [49:49<25:51,  1.20s/it]01/23/2020 11:10:57 - INFO - __main__ -   Epoch 1, step 2499, loss 7.439308166503906.\n",
            "01/23/2020 11:10:57 - INFO - __main__ -   Ground: [CLS]statusofthesecondphaseofthemagictelescope[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 11:10:57 - INFO - __main__ -   Generated: aofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP]------------------\n",
            "Train iter:  68%|██████▊   | 2599/3796 [51:48<23:45,  1.19s/it]01/23/2020 11:12:56 - INFO - __main__ -   Epoch 1, step 2599, loss 7.608412265777588.\n",
            "01/23/2020 11:12:56 - INFO - __main__ -   Ground: [CLS]highestweightcategoriesarisingfromk##ho##van##ov'sdiagramalgebraii:ko##sz##uli##ty[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 11:12:56 - INFO - __main__ -   Generated: aofofof[SEP]of[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]--------\n",
            "Train iter:  71%|███████   | 2699/3796 [53:47<21:49,  1.19s/it]01/23/2020 11:14:55 - INFO - __main__ -   Epoch 1, step 2699, loss 7.57956075668335.\n",
            "01/23/2020 11:14:55 - INFO - __main__ -   Ground: [CLS]u.s.stockmarketinteractionnetworkaslearnedbythebolt##zman##nmachine[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 11:14:55 - INFO - __main__ -   Generated: aaofofofof[SEP]ofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-----------\n",
            "Train iter:  74%|███████▎  | 2799/3796 [55:46<19:49,  1.19s/it]01/23/2020 11:16:55 - INFO - __main__ -   Epoch 1, step 2799, loss 7.3766398429870605.\n",
            "01/23/2020 11:16:55 - INFO - __main__ -   Ground: [CLS]non##para##metricmaximumentropyprobabilitydensityestimation[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 11:16:55 - INFO - __main__ -   Generated: theofofofofofof[SEP][SEP][SEP]-------------------\n",
            "Train iter:  76%|███████▋  | 2899/3796 [57:45<17:49,  1.19s/it]01/23/2020 11:18:54 - INFO - __main__ -   Epoch 1, step 2899, loss 7.441502571105957.\n",
            "01/23/2020 11:18:54 - INFO - __main__ -   Ground: [CLS]amodelofsensoryneuralresponsesinthepresenceofunknownmod##ulator##yinputs[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 11:18:54 - INFO - __main__ -   Generated: aofofofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]------------\n",
            "Train iter:  79%|███████▉  | 2999/3796 [59:45<15:47,  1.19s/it]01/23/2020 11:20:53 - INFO - __main__ -   Epoch 1, step 2999, loss 7.420760631561279.\n",
            "01/23/2020 11:20:53 - INFO - __main__ -   Ground: [CLS]converge##ntandanti-di##ff##us##ivepropertiesofmean-shiftmethod[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 11:20:53 - INFO - __main__ -   Generated: a--of---of[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]------------\n",
            "Train iter:  82%|████████▏ | 3099/3796 [1:01:44<13:50,  1.19s/it]01/23/2020 11:22:53 - INFO - __main__ -   Epoch 1, step 3099, loss 7.444163799285889.\n",
            "01/23/2020 11:22:53 - INFO - __main__ -   Ground: [CLS]localquasi-likelihoodwithapara##metricguide[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 11:22:53 - INFO - __main__ -   Generated: aofofofofof[SEP][SEP][SEP][SEP][SEP]------------------\n",
            "Train iter:  84%|████████▍ | 3199/3796 [1:03:43<11:51,  1.19s/it]01/23/2020 11:24:52 - INFO - __main__ -   Epoch 1, step 3199, loss 7.415365695953369.\n",
            "01/23/2020 11:24:52 - INFO - __main__ -   Ground: [CLS]qc##dfactor##izationapproachforrare$\\barb^0\\tod^*\\gamma$decay[SEP][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 11:24:52 - INFO - __main__ -   Generated: aofofof[SEP][SEP]ofofofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-----\n",
            "Train iter:  87%|████████▋ | 3299/3796 [1:05:43<09:52,  1.19s/it]01/23/2020 11:26:51 - INFO - __main__ -   Epoch 1, step 3299, loss 7.403223991394043.\n",
            "01/23/2020 11:26:51 - INFO - __main__ -   Ground: [CLS]informationgeometryfortestingpseudo##rand##omnumbergenerators[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 11:26:51 - INFO - __main__ -   Generated: theofofofofofof[SEP]of[SEP][SEP]------------------\n",
            "Train iter:  90%|████████▉ | 3399/3796 [1:07:42<07:52,  1.19s/it]01/23/2020 11:28:50 - INFO - __main__ -   Epoch 1, step 3399, loss 7.422475814819336.\n",
            "01/23/2020 11:28:50 - INFO - __main__ -   Ground: [CLS]languagescoolastheyexpand:all##ometricscalingandthedecreasingneedfornewwords[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 11:28:50 - INFO - __main__ -   Generated: aofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-----------\n",
            "Train iter:  92%|█████████▏| 3499/3796 [1:09:41<05:53,  1.19s/it]01/23/2020 11:30:49 - INFO - __main__ -   Epoch 1, step 3499, loss 7.511587142944336.\n",
            "01/23/2020 11:30:49 - INFO - __main__ -   Ground: [CLS]non-as##ym##pt##oticclosed-loopsystemidentificationusingauto##re##gre##ssi##veprocessesandhank##elmodelreduction[SEP][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 11:30:49 - INFO - __main__ -   Generated: atheof--[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]----\n",
            "Train iter:  95%|█████████▍| 3599/3796 [1:11:40<03:54,  1.19s/it]01/23/2020 11:32:49 - INFO - __main__ -   Epoch 1, step 3599, loss 7.5036163330078125.\n",
            "01/23/2020 11:32:49 - INFO - __main__ -   Ground: [CLS]controltheorem##sforabel##ianvarietiesoverfunctionfields[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 11:32:49 - INFO - __main__ -   Generated: aofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-----------------\n",
            "Train iter:  97%|█████████▋| 3699/3796 [1:13:40<01:56,  1.20s/it]01/23/2020 11:34:48 - INFO - __main__ -   Epoch 1, step 3699, loss 7.413100242614746.\n",
            "01/23/2020 11:34:48 - INFO - __main__ -   Ground: [CLS]thestudentensembleofcorrelationmatrices:e##igen##val##uespectrumandku##ll##back-lei##blerentropy[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 11:34:48 - INFO - __main__ -   Generated: aofofofofofof[SEP]of[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-------\n",
            "Train iter: 100%|██████████| 3796/3796 [1:15:36<00:00,  1.19s/it]\n",
            "01/23/2020 11:36:46 - INFO - __main__ -   Model saved\n",
            "01/23/2020 11:36:53 - INFO - __main__ -   Source: [CLS]petdetectorsprovidetheinformationoftheposition,theenergyandthetimingaboutthedecayeventsonthelo##r.traditionalpetimagereconstructionhasnottakenthetiminginformationintoaccount,onlyusedthetiminginformationforcoincidencejudgments.thehightimingresolutionpetdetectorsprovideverypreciseto##finformation,thento##fimagereconstructionmethodwhichutilizesthetiminginformationofpetdetectorsissocrucialfortheto##fpetsystem.wetakeadvantageoftiminginformationprovidedbyapairofto##fpetdetectors,andthenrec##ons##tructtheactivitydistributionfromthelimited-viewprojectiondata.sincetheimagereconstructionfromthelimited-viewdataisanunder-[SEP]\n",
            "01/23/2020 11:36:53 - INFO - __main__ -   Beam Generated: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
            "01/23/2020 11:36:53 - INFO - __main__ -   ***** Running validation *****\n",
            "Val iter: 100%|██████████| 422/422 [54:52<00:00,  7.80s/it]\n",
            "01/23/2020 12:31:46 - INFO - __main__ -   ******Validation results******\n",
            "01/23/2020 12:31:46 - INFO - __main__ -   Rouge-1: 0.0\n",
            "01/23/2020 12:31:46 - INFO - __main__ -   Rouge-2: 0.0\n",
            "01/23/2020 12:31:46 - INFO - __main__ -   Validation finished.\n",
            "01/23/2020 12:31:46 - INFO - __main__ -   Epoch 1 finished.\n",
            "Train iter:   3%|▎         | 99/3796 [01:58<1:13:28,  1.19s/it]01/23/2020 12:33:46 - INFO - __main__ -   Epoch 2, step 99, loss 7.438243865966797.\n",
            "01/23/2020 12:33:46 - INFO - __main__ -   Ground: [CLS]thompsonsamplingforcomplexbanditproblems[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 12:33:46 - INFO - __main__ -   Generated: a-ofof[SEP][SEP][SEP][SEP]---------------------\n",
            "Train iter:   5%|▌         | 199/3796 [03:57<1:11:37,  1.19s/it]01/23/2020 12:35:45 - INFO - __main__ -   Epoch 2, step 199, loss 7.372300624847412.\n",
            "01/23/2020 12:35:45 - INFO - __main__ -   Ground: [CLS]pro##ba##bilis##ticpredictionofneurologicaldisorderswithastatisticalassessmentofne##uro##ima##gingdatamod##ali##ties[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 12:35:45 - INFO - __main__ -   Generated: aaof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]------\n",
            "Train iter:   8%|▊         | 299/3796 [05:56<1:09:34,  1.19s/it]01/23/2020 12:37:44 - INFO - __main__ -   Epoch 2, step 299, loss 7.43074369430542.\n",
            "01/23/2020 12:37:44 - INFO - __main__ -   Ground: [CLS]super##con##du##ct##ivityinnovelge-basedsk##utter##udi##tes:{sr,ba}pt_4##ge_{12}[SEP]\n",
            "01/23/2020 12:37:44 - INFO - __main__ -   Generated: a[SEP]of[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]\n",
            "Train iter:  11%|█         | 399/3796 [07:55<1:07:26,  1.19s/it]01/23/2020 12:39:44 - INFO - __main__ -   Epoch 2, step 399, loss 7.502837181091309.\n",
            "01/23/2020 12:39:44 - INFO - __main__ -   Ground: [CLS]thecoe##vo##lu##tionofbanksandcorporatesecuritiesmarkets:thefinancingofbelgium'sindustrialtake-offinthe1830s[SEP][PAD][PAD][PAD]\n",
            "01/23/2020 12:39:44 - INFO - __main__ -   Generated: aaofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]--\n",
            "Train iter:  13%|█▎        | 499/3796 [09:55<1:05:48,  1.20s/it]01/23/2020 12:41:43 - INFO - __main__ -   Epoch 2, step 499, loss 7.425358295440674.\n",
            "01/23/2020 12:41:43 - INFO - __main__ -   Ground: [CLS]onlinedualcoordinateascentlearning[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 12:41:43 - INFO - __main__ -   Generated: atheofofofofof----------------------\n",
            "Train iter:  16%|█▌        | 599/3796 [11:54<1:03:28,  1.19s/it]01/23/2020 12:43:42 - INFO - __main__ -   Epoch 2, step 599, loss 7.520330905914307.\n",
            "01/23/2020 12:43:42 - INFO - __main__ -   Ground: [CLS]h^+_3w##z##n##wmodelfromli##ou##villefieldtheory[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 12:43:42 - INFO - __main__ -   Generated: a-ofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]-----------\n",
            "Train iter:  18%|█▊        | 699/3796 [13:53<1:01:31,  1.19s/it]01/23/2020 12:45:42 - INFO - __main__ -   Epoch 2, step 699, loss 7.5371880531311035.\n",
            "01/23/2020 12:45:42 - INFO - __main__ -   Ground: [CLS]exacthybridparticle/populationsimulationofrule-basedmodelsofbio##chemicalsystems[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 12:45:42 - INFO - __main__ -   Generated: aofofof[SEP][SEP]of[SEP]of[SEP][SEP]of[SEP][SEP]of[SEP][SEP]------------\n",
            "Train iter:  21%|██        | 799/3796 [15:53<59:36,  1.19s/it]01/23/2020 12:47:41 - INFO - __main__ -   Epoch 2, step 799, loss 7.600355625152588.\n",
            "01/23/2020 12:47:41 - INFO - __main__ -   Ground: [CLS]acomputationalmethodfortherateestimationofevolutionarytrans##position##s[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 12:47:41 - INFO - __main__ -   Generated: theofofofofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP]---------------\n",
            "Train iter:  24%|██▎       | 899/3796 [17:52<57:30,  1.19s/it]01/23/2020 12:49:40 - INFO - __main__ -   Epoch 2, step 899, loss 7.470128059387207.\n",
            "01/23/2020 12:49:40 - INFO - __main__ -   Ground: [CLS]makingconsensustract##able[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 12:49:40 - INFO - __main__ -   Generated: aofofof[SEP][SEP]-----------------------\n",
            "Train iter:  26%|██▋       | 999/3796 [19:52<55:42,  1.19s/it]01/23/2020 12:51:40 - INFO - __main__ -   Epoch 2, step 999, loss 7.849353313446045.\n",
            "01/23/2020 12:51:40 - INFO - __main__ -   Ground: [CLS]theimpactofsexeducationonsexualactivity,pregnancy,andabortion[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 12:51:40 - INFO - __main__ -   Generated: aofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]--------------\n",
            "Train iter:  29%|██▉       | 1099/3796 [21:51<53:44,  1.20s/it]01/23/2020 12:53:39 - INFO - __main__ -   Epoch 2, step 1099, loss 7.3882575035095215.\n",
            "01/23/2020 12:53:39 - INFO - __main__ -   Ground: [CLS]measuringandimplementingthebull##w##hipeffectunderageneralizeddemandprocess[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 12:53:39 - INFO - __main__ -   Generated: aof[SEP]of[SEP]ofofof[SEP][SEP][SEP][SEP][SEP][SEP][SEP]--------------\n",
            "Train iter:  32%|███▏      | 1199/3796 [23:51<51:38,  1.19s/it]01/23/2020 12:55:39 - INFO - __main__ -   Epoch 2, step 1199, loss 7.487708568572998.\n",
            "01/23/2020 12:55:39 - INFO - __main__ -   Ground: [CLS]multi-scalegraph-basedgradingforalzheimer'sdiseaseprediction[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 12:55:39 - INFO - __main__ -   Generated: thetheofofofofof[SEP]of[SEP][SEP][SEP][SEP][SEP][SEP]--------------\n",
            "Train iter:  34%|███▍      | 1299/3796 [25:50<50:02,  1.20s/it]01/23/2020 12:57:38 - INFO - __main__ -   Epoch 2, step 1299, loss 7.3974409103393555.\n",
            "01/23/2020 12:57:38 - INFO - __main__ -   Ground: [CLS]dependencyandfalsediscoveryrate:as##ym##pt##otic##s[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "01/23/2020 12:57:38 - INFO - __main__ -   Generated: a-of[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]of----------------\n",
            "Train iter:  37%|███▋      | 1386/3796 [27:35<48:15,  1.20s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL3gA01uHlar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ls /content/bertsum/output/model_01-13-05:43:53 -la\n",
        "#!cp /content/bertsum/output/model_01-13-05:43:53/BertAbsSum_1.bin /content/drive/My\\ Drive/nlp/BertAbsSum_11.bin\n",
        "# !cp /content/bertsum/output/model_01-13-05:43:53/config.json /content/drive/My\\ Drive/nlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CADWqILgRlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHNngTAigSk9",
        "colab_type": "text"
      },
      "source": [
        "##Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAM_OuvejGqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzXzlhLAgyQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ARGS(object):\n",
        "    # data_dir = 'data/processed_data'\n",
        "    bert_model = 'bert-base-uncased'\n",
        "    output_dir = 'output'\n",
        "    model_path =  'output/model_01-13-05:43:53/BertAbsSum_1.bin'\n",
        "    config_path = 'output/model_01-13-05:43:53/config.json'\n",
        "    result_path = 'result'\n",
        "    batch_size = 16\n",
        "    max_src_len = 130\n",
        "    \n",
        "    # GPU_index = 0\n",
        "    # learning_rate = 5e-5\n",
        "    # num_train_epochs = 3\n",
        "    # warmup_proportion = 0.1\n",
        "    # max_src_len = 130\n",
        "    # max_tgt_len = 30\n",
        "    # train_batch_size = 16\n",
        "    # decoder_config = None\n",
        "    # print_every = 100\n",
        "    # gradient_accumulation_steps = 1\n",
        "\n",
        "\n",
        "args = ARGS()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj8Ez_N4n3u7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "with open(args.config_path, 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "model = BertAbsSum(args.bert_model, config['decoder_config'], device)\n",
        "model.load_state_dict(torch.load(args.model_path))\n",
        "model.to(device)\n",
        "\n",
        "# processor = CSVProcessor()\n",
        "# tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
        "# tokenizer = BertTokenizer.from_pretrained(os.path.join(args.bert_model, 'vocab.txt'))\n",
        "\n",
        "# test_examples = processor.get_test_examples(args.valid_path)\n",
        "# test_features = convert_examples_to_features(test_examples, args.max_src_len, args.max_tgt_len, tokenizer)\n",
        "# test_data = create_dataset(test_features)\n",
        "# test_sampler = RandomSampler(test_data)\n",
        "# test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE, drop_last=True)\n",
        "# logger.info('Loading complete. Writing results to %s' % (args.result_path))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUtdOFTIgRi3",
        "colab_type": "code",
        "outputId": "8f66da5d-5cb2-42ff-d25f-ecb572c3fb3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:   0%|          | 4/838 [00:05<18:44,  1.35s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-e2559f5cf085>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Iteration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/bertsum/model.py\u001b[0m in \u001b[0;36mbeam_decode\u001b[0;34m(self, src_seq, src_mask, beam_size, n_best)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 active_inst_idx_list = beam_decode_step(\n\u001b[0;32m--> 201\u001b[0;31m                     inst_dec_beams, len_dec_seq, src_seq, src_enc, inst_idx_to_position_map, beam_size)\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mactive_inst_idx_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/bertsum/model.py\u001b[0m in \u001b[0;36mbeam_decode_step\u001b[0;34m(inst_dec_beams, len_dec_seq, src_seq, enc_output, inst_idx_to_position_map, beam_size)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mdec_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_beam_dec_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst_dec_beams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_dec_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mword_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_active_inst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# Update the beam with predicted word prob information and collect incomplete instances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/bertsum/model.py\u001b[0m in \u001b[0;36mpredict_word\u001b[0;34m(dec_seq, src_seq, enc_output, n_active_inst, beam_size)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mpredict_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_active_inst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mdec_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m                 \u001b[0mdec_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Pick the last step: (bh * bm) * d_h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mword_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/bertsum/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt_seq, src_seq, enc_output)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mdec_enc_attn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_attn_key_pad_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_q\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mtgt_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;31m# -- Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mdec_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_seq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlMFUL-HtJuR",
        "colab_type": "code",
        "outputId": "db9da36d-812c-4d78-ce87-0e02783a696c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6798116839783025"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpn-Cj9ms3hf",
        "colab_type": "code",
        "outputId": "ea8ba79a-01b2-4187-c09a-b6d7a9859856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(hyp_list), len(ref_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13408 13408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M85q_0yll7_",
        "colab_type": "text"
      },
      "source": [
        "##Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3rb1QF9mMrB",
        "colab_type": "text"
      },
      "source": [
        "###Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvn5wKk-mFlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ARGS(object):\n",
        "    # data_dir = 'data/processed_data'\n",
        "    bert_model = 'bert-base-uncased'\n",
        "    output_dir = 'output'\n",
        "    model_path =  'output/model_01-13-05:43:53/BertAbsSum_1.bin'\n",
        "    config_path = 'output/model_01-13-05:43:53/config.json'\n",
        "    result_path = 'result'\n",
        "    batch_size = 16\n",
        "    max_src_len = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W46pBZXuX6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "with open(args.config_path, 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "model = BertAbsSum(args.bert_model, config['decoder_config'], device)\n",
        "model.load_state_dict(torch.load(args.model_path))\n",
        "model.to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gio6mNTmmQR-",
        "colab_type": "text"
      },
      "source": [
        "###Generate titles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9Bti4I4ljz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "# test = pd.read_csv('../data/test.csv', encoding='utf8')\n",
        "\n",
        "titles = []\n",
        "\n",
        "for row in tqdm(test.iterrows(), desc=\"Iteration\", total = test.shape[0], position=0):\n",
        "  i, text = row\n",
        "  src, src_mask = convert_one_example(text[0], args.max_src_len, tokenizer)\n",
        "  pred, _ = model.beam_decode(src.to(device), src_mask.to(device), 3, 3)  \n",
        "  # print(\" \".join(tokenizer.convert_ids_to_tokens(pred[0][0])).split('[SEP]')[0])\n",
        "\n",
        "  # De-tokenize.\n",
        "  tok_text = \" \".join(tokenizer.convert_ids_to_tokens(pred[0][0])).split('[SEP]')[0]\n",
        "  tok_text = tok_text.replace(\" ##\", \"\")\n",
        "  tok_text = tok_text.replace(\"##\", \"\")  \n",
        "  \n",
        "  # tok_text = ''\n",
        "  # for t in tokenizer.convert_ids_to_tokens(pred[0][0]):\n",
        "  #   tok_text += ' ' + t if not t.startswith('##') else t[2:]\n",
        "  # tok_text = tok_text.split('[SEP]')[0][1:]\n",
        "  titles.append(tok_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUli0ZLWljxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_df = pd.DataFrame({'abstract': test.abstract, 'title': titles})\n",
        "submission_df.to_csv('predicted_titles.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVgoREBHmV5L",
        "colab_type": "text"
      },
      "source": [
        "###Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhR5PhD9l1Hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "from nltk.util import ngrams\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "\n",
        "def generate_csv(input_file='predicted_titles.csv',\n",
        "                 output_file='submission.csv',\n",
        "                 voc_file='../data/vocs.pkl'):\n",
        "    '''\n",
        "    Generates file in format required for submitting result to Kaggle\n",
        "    \n",
        "    Parameters:\n",
        "        input_file (str) : path to csv file with your predicted titles.\n",
        "                           Should have two fields: abstract and title\n",
        "        output_file (str) : path to output submission file\n",
        "        voc_file (str) : path to voc.pkl file\n",
        "    '''\n",
        "    data = pd.read_csv(input_file)\n",
        "    with open(voc_file, 'rb') as voc_file:\n",
        "        vocs = pickle.load(voc_file)\n",
        "\n",
        "    with open(output_file, 'w') as res_file:\n",
        "        res_file.write('Id,Predict\\n')\n",
        "        \n",
        "    output_idx = 0\n",
        "    for row_idx, row in data.iterrows():\n",
        "        trg = row['title']\n",
        "        trg = trg.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
        "        trg.extend(['_'.join(ngram) for ngram in list(ngrams(trg, 2)) + list(ngrams(trg, 3))])\n",
        "        \n",
        "        VOCAB_stoi = vocs[row_idx]\n",
        "        trg_intersection = set(VOCAB_stoi.keys()).intersection(set(trg))\n",
        "        trg_vec = np.zeros(len(VOCAB_stoi))    \n",
        "\n",
        "        for word in trg_intersection:\n",
        "            trg_vec[VOCAB_stoi[word]] = 1\n",
        "\n",
        "        with open(output_file, 'a') as res_file:\n",
        "            for is_word in trg_vec:\n",
        "                res_file.write('{0},{1}\\n'.format(output_idx, int(is_word)))\n",
        "                output_idx += 1\n",
        "\n",
        "\n",
        "generate_csv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHHG7mRcl1OO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIF7JirNl1Ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjVt5fNxmcUp",
        "colab_type": "text"
      },
      "source": [
        "#Trash"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZS3pEsLljtm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l-ynDMcvOlD",
        "colab_type": "code",
        "outputId": "e8d0b5b8-08a7-4518-d83a-be47eb21fb33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "test_examples = processor.get_test_examples('../data/test.csv')\n",
        "\n",
        "test_features = convert_examples_to_features(test_examples, args.max_src_len, args.max_tgt_len, tokenizer)\n",
        "test_data = create_dataset(test_features)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=args.batch_size, drop_last=True)\n",
        "logger.info('Loading test data complete.')\n",
        "\n",
        "\n",
        "#test_dataloader"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-d60b8b9544ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_test_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_examples_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_src_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_tgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ARGS' object has no attribute 'max_src_len'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eaL_85SvK9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for batch in tqdm(test_dataloader, desc=\"Iteration\", position=0):\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    pred, _ = model.beam_decode(batch[0], batch[1], 3, 3)\n",
        "    src, tgt = batch[0], batch[2]\n",
        "    for i in range(args.batch_size):\n",
        "        sample_src = \"\".join(tokenizer.convert_ids_to_tokens(src[i].cpu().numpy())).split('[CLS]')[1].split('[SEP]')[0] + '\\n'\n",
        "        sample_tgt = \"\".join(tokenizer.convert_ids_to_tokens(tgt[i].cpu().numpy())).split('[CLS]')[1].split('[SEP]')[0] + '\\n'\n",
        "        sample_pred = \"\".join(tokenizer.convert_ids_to_tokens(pred[i][0])).split('[SEP]')[0] + '\\n'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13eh_d6k0r4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "from nltk.util import ngrams\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "with open('../data/vocs.pkl', 'rb') as voc_file:\n",
        "    vocs = pickle.load(voc_file)\n",
        "\n",
        "\n",
        "output_idx = 0\n",
        "for row_idx, text in test[:10].iterrows():\n",
        "  \n",
        "  src, src_mask = convert_one_example(text[0], args.max_src_len, tokenizer)\n",
        "  pred, _ = model.beam_decode(src.to(device), src_mask.to(device), 3, 3)\n",
        "  trg = \"\".join(tokenizer.convert_ids_to_tokens(pred[0][0])).split('[SEP]')[0]\n",
        "  # trg = row['title']\n",
        "  print(trg)\n",
        "  trg = trg.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
        "  print(trg)\n",
        "  trg.extend(['_'.join(ngram) for ngram in list(ngrams(trg, 2)) + list(ngrams(trg, 3))])\n",
        "  \n",
        "  VOCAB_stoi = vocs[row_idx]\n",
        "  trg_intersection = set(VOCAB_stoi.keys()).intersection(set(trg))\n",
        "  trg_vec = np.zeros(len(VOCAB_stoi))    \n",
        "\n",
        "  for word in trg_intersection:\n",
        "      trg_vec[VOCAB_stoi[word]] = 1\n",
        "\n",
        "  for is_word in trg_vec:\n",
        "      #res_file.write('{0},{1}\\n'.format(output_idx, int(is_word)))\n",
        "      print('{0},{1}\\n'.format(output_idx, int(is_word)))\n",
        "      output_idx += 1\n",
        "\n",
        "  print('')      \n",
        "\n",
        "  # with open(output_file, 'a') as res_file:\n",
        "  #     for is_word in trg_vec:\n",
        "  #         res_file.write('{0},{1}\\n'.format(output_idx, int(is_word)))\n",
        "  #         output_idx += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ64P3nB-ZEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "titles = []\n",
        "for abstract in abstracts:\n",
        "    title, _ = translate_sentence(model, abstract.split())\n",
        "    titles.append(' '.join(title).replace('<unk>', ''))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV_z48okln0y",
        "colab_type": "code",
        "outputId": "b6c98501-22b7-43c2-d6c1-f2b4e90b187c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "# test = pd.read_csv('../data/test.csv', encoding='utf8')\n",
        "\n",
        "titles = []\n",
        "\n",
        "for row in tqdm(test.iterrows(), desc=\"Iteration\", total = test.shape[0], position=0):\n",
        "  i, text = row\n",
        "  src, src_mask = convert_one_example(text[0], args.max_src_len, tokenizer)\n",
        "  pred, _ = model.beam_decode(src.to(device), src_mask.to(device), 3, 3)\n",
        "  # print(\" \".join(tokenizer.convert_ids_to_tokens(pred[0][0])).split('[SEP]')[0])\n",
        "  s = ''\n",
        "  for t in tokenizer.convert_ids_to_tokens(pred[0][0]):\n",
        "    s += ' ' + t if not t.startswith('##') else t[2:]\n",
        "  s = s.split('[SEP]')[0][1:]\n",
        "  titles.append(s)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 1000it [05:12,  2.82it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1K1X7B2-vhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_df = pd.DataFrame({'abstract': test.abstract, 'title': titles})\n",
        "submission_df.to_csv('predicted_titles.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGDsz4Pp0VNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HMaLLNXBH-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}