{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "BertAbsSum2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "U7h5IEsJfJs7"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brs1977/BERT-Transformer-for-Summarization/blob/master/BertAbsSum2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DKufDQceuaD",
        "colab_type": "code",
        "outputId": "a297801f-5e3e-4bcb-ea7f-b663b492d581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7h5IEsJfJs7",
        "colab_type": "text"
      },
      "source": [
        "##Get Kaggle data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ7V-uhSfIYk",
        "colab_type": "code",
        "outputId": "458c0b73-0702-43d9-dd9e-c183079ec03b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#!pip install -q kaggle\n",
        "\n",
        "#kaggle key\n",
        "!mkdir ~/.kaggle\n",
        "!cp /content/drive/My\\ Drive/kaggle.json ~/.kaggle\n",
        "!ls ~/.kaggle"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h32t1QHfQk8",
        "colab_type": "code",
        "outputId": "7a98563c-09d5-4272-dbe6-d12e0d9068a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "!kaggle competitions download -c title-generation"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading vocs.pkl.zip to /content\n",
            " 78% 5.00M/6.39M [00:00<00:00, 45.3MB/s]\n",
            "100% 6.39M/6.39M [00:00<00:00, 41.0MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/778k [00:00<?, ?B/s]\n",
            "100% 778k/778k [00:00<00:00, 244MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 80% 35.0M/43.7M [00:00<00:00, 36.7MB/s]\n",
            "100% 43.7M/43.7M [00:00<00:00, 74.6MB/s]\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/905k [00:00<?, ?B/s]\n",
            "100% 905k/905k [00:00<00:00, 126MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpltIhc-fVqS",
        "colab_type": "code",
        "outputId": "8ecb5150-7c24-4ca1-8386-ba66c89f77ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!mkdir data\n",
        "!unzip sample_submission.csv.zip -d data\n",
        "!unzip vocs.pkl.zip -d data\n",
        "!unzip train.csv.zip -d data\n",
        "!mv test.csv data\n",
        "\n",
        "!ls data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: data/sample_submission.csv  \n",
            "Archive:  vocs.pkl.zip\n",
            "  inflating: data/vocs.pkl           \n",
            "Archive:  train.csv.zip\n",
            "  inflating: data/train.csv          \n",
            "sample_submission.csv  test.csv  train.csv  vocs.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiawlFnGfbV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv('data/train.csv', encoding='utf8')\n",
        "test = pd.read_csv('data/test.csv', encoding='utf8')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0288Y3gPfc8a",
        "colab_type": "code",
        "outputId": "e0b8b8a7-b169-4d48-e77d-45b6051d3eb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>we consider the problem of utility maximizatio...</td>\n",
              "      <td>on optimal investment with processes of long o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>in this paper we provide an explicit formula f...</td>\n",
              "      <td>boolean complexes for ferrers graphs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>kinesin-5, also known as eg5 in vertebrates is...</td>\n",
              "      <td>relative velocity of sliding of microtubules b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>we discuss the transition paths in a coupled b...</td>\n",
              "      <td>bifurcation of transition paths induced by cou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>two types of room temperature detectors of ter...</td>\n",
              "      <td>all-electric detectors of the polarization sta...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            abstract                                              title\n",
              "0  we consider the problem of utility maximizatio...  on optimal investment with processes of long o...\n",
              "1  in this paper we provide an explicit formula f...               boolean complexes for ferrers graphs\n",
              "2  kinesin-5, also known as eg5 in vertebrates is...  relative velocity of sliding of microtubules b...\n",
              "3  we discuss the transition paths in a coupled b...  bifurcation of transition paths induced by cou...\n",
              "4  two types of room temperature detectors of ter...  all-electric detectors of the polarization sta..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CjNvi4nfhFc",
        "colab_type": "text"
      },
      "source": [
        "##Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWO9FdL0fB5v",
        "colab_type": "code",
        "outputId": "c1cad62f-7628-4ec6-dd00-ca0e86c3751b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "%cd /content\n",
        "!rm bertsum -r\n",
        "!git clone https://github.com/brs1977/BERT-Transformer-for-Summarization bertsum\n",
        "\n",
        "# !git pull origin master"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "rm: cannot remove 'bertsum': No such file or directory\n",
            "Cloning into 'bertsum'...\n",
            "remote: Enumerating objects: 105, done.\u001b[K\n",
            "remote: Counting objects: 100% (105/105), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 105 (delta 47), reused 76 (delta 27), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (105/105), 107.04 KiB | 3.15 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq77ji8Wow4T",
        "colab_type": "code",
        "outputId": "12ce746c-22fa-4ba9-f253-9690f1cce049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/bertsum"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/bertsum\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvZ-_YvEeQLk",
        "colab_type": "code",
        "outputId": "b2f11bf3-bdc1-4b43-a1d7-4459cf44d599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/bertsum\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "from preprocess import CSVProcessor, create_dataset\n",
        "from model import BertAbsSum\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "from pytorch_pretrained_bert.optimization import BertAdam\n",
        "from preprocess import convert_examples_to_features\n",
        "from tqdm import tqdm, trange\n",
        "from transformer import Constants"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/bertsum\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYvyMXe7f69f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB-lgaRTf7A1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBJiX709huHD",
        "colab_type": "text"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuFtKU0eiDwi",
        "colab_type": "text"
      },
      "source": [
        "###Bert config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRj0gX06eQLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                    level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uelhDbCjeQLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQBqPZK9eQLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ARGS(object):\n",
        "    data_dir = 'data/processed_data'\n",
        "    bert_model = 'bert-base-uncased'\n",
        "    #output_dir = 'output'\n",
        "    output_dir = '/content/drive/My Drive/nlp'\n",
        "    GPU_index = 0\n",
        "    learning_rate = 5e-3\n",
        "    num_train_epochs = 3\n",
        "    warmup_proportion = 0.1\n",
        "    max_src_len = 130\n",
        "    max_tgt_len = 30\n",
        "    train_batch_size = 32\n",
        "    valid_batch_size = 32\n",
        "    decoder_config = None\n",
        "    print_every = 100\n",
        "    gradient_accumulation_steps = 1\n",
        "\n",
        "\n",
        "args = ARGS()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH4VXzObeQLx",
        "colab_type": "code",
        "outputId": "c96a2632-491a-4b71-bce2-c0261a0de66d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda', args.GPU_index)\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "logger.info(f'Using device:{device}')\n",
        "\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)\n",
        "model_path = os.path.join(args.output_dir, time.strftime('model_%m-%d-%H:%M:%S', time.localtime()))\n",
        "os.mkdir(model_path)\n",
        "logger.info(f'Saving model to {model_path}.')\n",
        "\n",
        "if args.decoder_config is not None:\n",
        "    with open(args.decoder_config, 'r') as f:\n",
        "        decoder_config = json.load(f)\n",
        "else:\n",
        "    with open(os.path.join(args.bert_model, 'bert_config.json'), 'r') as f:\n",
        "        bert_config = json.load(f)\n",
        "        decoder_config = {}\n",
        "        decoder_config = {}\n",
        "        decoder_config['len_max_seq'] = args.max_tgt_len\n",
        "        decoder_config['d_word_vec'] = bert_config['hidden_size']\n",
        "        decoder_config['n_layers'] = 8\n",
        "        decoder_config['n_head'] = 12\n",
        "        decoder_config['d_k'] = 64\n",
        "        decoder_config['d_v'] = 64\n",
        "        decoder_config['d_model'] = bert_config['hidden_size']\n",
        "        decoder_config['d_inner'] = bert_config['hidden_size']\n",
        "        decoder_config['vocab_size'] = bert_config['vocab_size']        "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01/23/2020 08:06:16 - INFO - __main__ -   Using device:cuda:0\n",
            "01/23/2020 08:06:16 - INFO - __main__ -   Saving model to /content/drive/My Drive/nlp/model_01-23-08:06:16.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE6VE9npiQ6t",
        "colab_type": "text"
      },
      "source": [
        "###Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGqDLLzJO1hq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "58d7fc8d-8d05-41b4-849b-0d5be04b3e7c"
      },
      "source": [
        "#prepared data\n",
        "train_data  = torch.load('/content/drive/My Drive/nlp/nlp_model/abs_bert/data130X30/train.pt')\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size, drop_last=True)\n",
        "\n",
        "valid_data = torch.load('/content/drive/My Drive/nlp/nlp_model/abs_bert/data130X30/valid.pt')\n",
        "valid_sampler = RandomSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=args.valid_batch_size, drop_last=True)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
        "\n",
        "\n",
        "# torch.save(train_data,'train.pt')\n",
        "# torch.save(valid_data,'valid.pt')\n",
        "# !cp *.pt /content/drive/My\\ Drive/nlp/nlp_model/abs_bert/data130X30\n",
        "# !ls /content/drive/My\\ Drive/nlp/nlp_model/abs_bert/data130X30 -la\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01/23/2020 08:07:09 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uok4w6IzVo7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "is_test = False\n",
        "nrows = None if not is_test else 500\n",
        "\n",
        "# data preprocess\n",
        "processor = CSVProcessor()\n",
        "tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
        "# tokenizer = BertTokenizer.from_pretrained(os.path.join(args.bert_model, 'vocab.txt'))\n",
        "logger.info('Loading train examples...')\n",
        "train_examples = processor.get_train_examples('../data/train.csv', nrows = nrows)\n",
        "num_train_optimization_steps = int(len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n",
        "logger.info('Converting train examples to features...')\n",
        "train_features = convert_examples_to_features(train_examples, args.max_src_len, args.max_tgt_len, tokenizer)\n",
        "example = train_examples[0]\n",
        "example_feature = train_features[0]\n",
        "logger.info(\"*** Example ***\")\n",
        "logger.info(\"guid: %s\" % (example.guid))\n",
        "logger.info(\"src text: %s\" % example.src)\n",
        "logger.info(\"src_ids: %s\" % \" \".join([str(x) for x in example_feature.src_ids]))\n",
        "logger.info(\"src_mask: %s\" % \" \".join([str(x) for x in example_feature.src_mask]))\n",
        "logger.info(\"tgt text: %s\" % example.tgt)\n",
        "logger.info(\"tgt_ids: %s\" % \" \".join([str(x) for x in example_feature.tgt_ids]))\n",
        "logger.info(\"tgt_mask: %s\" % \" \".join([str(x) for x in example_feature.tgt_mask]))\n",
        "logger.info('Building dataloader...')\n",
        "train_data = create_dataset(train_features)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size, drop_last=True)\n",
        "\n",
        "valid_examples = processor.get_valid_examples('../data/train.csv')\n",
        "logger.info('Converting valid examples to features...')\n",
        "valid_features = convert_examples_to_features(valid_examples, args.max_src_len, args.max_tgt_len, tokenizer)\n",
        "valid_data = create_dataset(valid_features)\n",
        "valid_sampler = RandomSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=args.train_batch_size, drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgeH3YrqWuvI",
        "colab_type": "code",
        "outputId": "88c8f9d2-a29d-4d0c-d1ca-4f3f1c604ca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8973"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNny66w7ioT7",
        "colab_type": "text"
      },
      "source": [
        "###Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx8bd6jR3C_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_performance(logits, ground, smoothing=True):\n",
        "    ground = ground[:, 1:]\n",
        "    logits = logits.view(-1, logits.size(-1))\n",
        "    ground = ground.contiguous().view(-1)\n",
        "\n",
        "    loss = cal_loss(logits, ground, smoothing=smoothing)\n",
        "\n",
        "    pad_mask = ground.ne(Constants.PAD)\n",
        "    pred = logits.max(-1)[1]\n",
        "    correct = pred.eq(ground)\n",
        "    correct = correct.masked_select(pad_mask).sum().item()\n",
        "    return loss, correct\n",
        "\n",
        "def cal_loss(logits, ground, smoothing=True):\n",
        "    def label_smoothing(logits, labels):\n",
        "        eps = 0.1\n",
        "        num_classes = logits.size(-1)\n",
        "\n",
        "        # >>> z = torch.zeros(2, 4).scatter_(1, torch.tensor([[2], [3]]), 1.23)\n",
        "        # >>> z\n",
        "        # tensor([[ 0.0000,  0.0000,  1.2300,  0.0000],\n",
        "        #        [ 0.0000,  0.0000,  0.0000,  1.2300]])\n",
        "        one_hot = torch.zeros_like(logits).scatter(1, labels.view(-1, 1), 1)\n",
        "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (num_classes - 1)\n",
        "        log_prb = F.log_softmax(logits, dim=1)\n",
        "        non_pad_mask = ground.ne(Constants.PAD)\n",
        "        loss = -(one_hot * log_prb).sum(dim=1)\n",
        "        loss = loss.masked_select(non_pad_mask).mean()\n",
        "        return loss\n",
        "    if smoothing:\n",
        "        loss = label_smoothing(logits, ground)\n",
        "    else:\n",
        "        loss = F.cross_entropy(logits, ground, ignore_index=Constants.PAD)\n",
        "    \n",
        "    return loss    \n",
        "\n",
        "def rouge(hyp, ref, n):\n",
        "    scores = []\n",
        "    for h, r in zip(hyp, ref):\n",
        "        r = re.sub(r'[UNK]', '', r)\n",
        "        r = re.sub(r'[’!\"#$%&\\'()*+,-./:：？！《》;<=>?@[\\\\]^_`{|}~]+', '', r)\n",
        "        r = re.sub(r'\\d', '', r)\n",
        "        r = re.sub(r'[a-zA-Z]', '', r)\n",
        "        count = 0\n",
        "        match = 0\n",
        "        for i in range(len(r) - n):\n",
        "            gram = r[i:i + n]\n",
        "            if gram in h:\n",
        "                match += 1\n",
        "            count += 1\n",
        "        scores.append(0 if count==0 else match / count)\n",
        "    return np.average(scores)\n",
        "\n",
        "def convert_one_example(text, src_max_seq_length, tokenizer):\n",
        "    src_tokens = tokenizer.tokenize(text)\n",
        "    if len(src_tokens) > src_max_seq_length - 2:\n",
        "        src_tokens = src_tokens[:(src_max_seq_length - 2)]\n",
        "    src_tokens = [\"[CLS]\"] + src_tokens + [\"[SEP]\"]\n",
        "\n",
        "    src_ids = tokenizer.convert_tokens_to_ids(src_tokens)\n",
        "\n",
        "    src_mask = [1] * len(src_ids)\n",
        "    src_padding = [0] * (src_max_seq_length - len(src_ids))\n",
        "    src_ids += src_padding\n",
        "    src_mask += src_padding\n",
        "\n",
        "    return torch.tensor([src_ids]), torch.tensor([src_mask])    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1dhUYPgjhW3",
        "colab_type": "text"
      },
      "source": [
        "###Validation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSNeu5s1jlGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "def do_validate():\n",
        "  logger.info(\"***** Running validation *****\")\n",
        "  model.eval()\n",
        "  hyp_list = []\n",
        "  ref_list = []\n",
        "  i = 0\n",
        "  for batch in tqdm(valid_dataloader, desc=\"Val iter\", position=0):\n",
        "      i += 1\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      pred, _ = model.beam_decode(batch[0], batch[1], 3, 3)\n",
        "      src, tgt = batch[0], batch[2]\n",
        "      for i in range(args.train_batch_size):\n",
        "          sample_src = \"\".join(tokenizer.convert_ids_to_tokens(src[i].cpu().numpy())).split('[CLS]')[1].split('[SEP]')[0] + '\\n'\n",
        "          sample_tgt = \"\".join(tokenizer.convert_ids_to_tokens(tgt[i].cpu().numpy())).split('[CLS]')[1].split('[SEP]')[0] + '\\n'\n",
        "          sample_pred = \"\".join(tokenizer.convert_ids_to_tokens(pred[i][0])).split('[SEP]')[0] + '\\n'\n",
        "\n",
        "          hyp_list.append(sample_pred)\n",
        "          ref_list.append(sample_tgt)\n",
        "  rouge_1 = rouge(hyp_list, ref_list, 1)\n",
        "  rouge_2 = rouge(hyp_list, ref_list, 2)\n",
        "  logger.info('******Validation results******')\n",
        "  logger.info(f'Rouge-1: {rouge_1}')\n",
        "  logger.info(f'Rouge-2: {rouge_2}')\n",
        "  logger.info('Validation finished.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtGK6INWjlKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLh3CyBgiZKB",
        "colab_type": "text"
      },
      "source": [
        "###Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7Q9fMBjsBHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model\n",
        "model = BertAbsSum(args.bert_model, decoder_config, device=device )\n",
        "model.to(device)\n",
        "\n",
        "# optimizer\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=args.learning_rate,\n",
        "                     warmup=0.1,\n",
        "                     t_total=num_train_optimization_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-4vlmfJqjSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPYc6qiFit8b",
        "colab_type": "text"
      },
      "source": [
        "###Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QD9HwPe3DDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training\n",
        "logger.info(\"***** Running training *****\")\n",
        "logger.info(\"  Num examples = %d\", len(train_examples))\n",
        "logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
        "logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
        "model.train()\n",
        "global_step = 0\n",
        "for i in range(int(args.num_train_epochs)):\n",
        "    # do training\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Train iter\", position=0)):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        logits = model(*batch)\n",
        "        loss, _ = cal_performance(logits, batch[2])\n",
        "\n",
        "        if args.gradient_accumulation_steps > 1:\n",
        "            loss = loss / args.gradient_accumulation_steps\n",
        "        loss.backward()\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += batch[0].size(0)\n",
        "        nb_tr_steps += 1\n",
        "        if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "        if (step + 1) % args.print_every == 0:\n",
        "            logger.info(f'Epoch {i}, step {step}, loss {loss.item()}.')\n",
        "            logger.info(f'Ground: {\"\".join(tokenizer.convert_ids_to_tokens(batch[2][0].cpu().numpy()))}')\n",
        "            logger.info(f'Generated: {\"\".join(tokenizer.convert_ids_to_tokens(logits[0].max(-1)[1].cpu().numpy()))}')\n",
        "    \n",
        "    #do save model\n",
        "    if args.output_dir is not None:\n",
        "        state_dict = model.state_dict()\n",
        "        torch.save(state_dict, os.path.join(model_path, 'BertAbsSum_{}.bin'.format(i)))\n",
        "        logger.info('Model saved')\n",
        "    \n",
        "    # do evaluation\n",
        "    if valid_dataloader is not None:\n",
        "        model.eval()\n",
        "        batch = next(iter(valid_dataloader))\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # beam_decode\n",
        "        pred, _ = model.beam_decode(batch[0], batch[1], 3, 3)\n",
        "        # pred = model.greedy_decode(batch[0], batch[1])\n",
        "        logger.info(f'Source: {\"\".join(tokenizer.convert_ids_to_tokens(batch[0][0].cpu().numpy()))}')\n",
        "        logger.info(f'Beam Generated: {\"\".join(tokenizer.convert_ids_to_tokens(pred[0][0]))}')\n",
        "        # logger.info(f'Beam Generated: {tokenizer.convert_ids_to_tokens(pred[0].cpu().numpy())}')\n",
        "    \n",
        "    # do validate        \n",
        "    do_validate()\n",
        "\n",
        "    logger.info(f'Epoch {i} finished.')\n",
        "with open(os.path.join(args.bert_model, 'bert_config.json'), 'r') as f:\n",
        "    bert_config = json.load(f)\n",
        "config = {'bert_config': bert_config, 'decoder_config': decoder_config}\n",
        "with open(os.path.join(model_path, 'config.json'), 'w') as f:\n",
        "    json.dump(config, f)\n",
        "logger.info('Training finished')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL3gA01uHlar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ls /content/bertsum/output/model_01-13-05:43:53 -la\n",
        "#!cp /content/bertsum/output/model_01-13-05:43:53/BertAbsSum_1.bin /content/drive/My\\ Drive/nlp/BertAbsSum_11.bin\n",
        "# !cp /content/bertsum/output/model_01-13-05:43:53/config.json /content/drive/My\\ Drive/nlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CADWqILgRlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHNngTAigSk9",
        "colab_type": "text"
      },
      "source": [
        "##Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAM_OuvejGqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzXzlhLAgyQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ARGS(object):\n",
        "    # data_dir = 'data/processed_data'\n",
        "    bert_model = 'bert-base-uncased'\n",
        "    output_dir = 'output'\n",
        "    model_path =  'output/model_01-13-05:43:53/BertAbsSum_1.bin'\n",
        "    config_path = 'output/model_01-13-05:43:53/config.json'\n",
        "    result_path = 'result'\n",
        "    batch_size = 16\n",
        "    max_src_len = 130\n",
        "    \n",
        "    # GPU_index = 0\n",
        "    # learning_rate = 5e-5\n",
        "    # num_train_epochs = 3\n",
        "    # warmup_proportion = 0.1\n",
        "    # max_src_len = 130\n",
        "    # max_tgt_len = 30\n",
        "    # train_batch_size = 16\n",
        "    # decoder_config = None\n",
        "    # print_every = 100\n",
        "    # gradient_accumulation_steps = 1\n",
        "\n",
        "\n",
        "args = ARGS()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj8Ez_N4n3u7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "with open(args.config_path, 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "model = BertAbsSum(args.bert_model, config['decoder_config'], device)\n",
        "model.load_state_dict(torch.load(args.model_path))\n",
        "model.to(device)\n",
        "\n",
        "# processor = CSVProcessor()\n",
        "# tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
        "# tokenizer = BertTokenizer.from_pretrained(os.path.join(args.bert_model, 'vocab.txt'))\n",
        "\n",
        "# test_examples = processor.get_test_examples(args.valid_path)\n",
        "# test_features = convert_examples_to_features(test_examples, args.max_src_len, args.max_tgt_len, tokenizer)\n",
        "# test_data = create_dataset(test_features)\n",
        "# test_sampler = RandomSampler(test_data)\n",
        "# test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE, drop_last=True)\n",
        "# logger.info('Loading complete. Writing results to %s' % (args.result_path))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUtdOFTIgRi3",
        "colab_type": "code",
        "outputId": "8f66da5d-5cb2-42ff-d25f-ecb572c3fb3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:   0%|          | 4/838 [00:05<18:44,  1.35s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-e2559f5cf085>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Iteration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/bertsum/model.py\u001b[0m in \u001b[0;36mbeam_decode\u001b[0;34m(self, src_seq, src_mask, beam_size, n_best)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 active_inst_idx_list = beam_decode_step(\n\u001b[0;32m--> 201\u001b[0;31m                     inst_dec_beams, len_dec_seq, src_seq, src_enc, inst_idx_to_position_map, beam_size)\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mactive_inst_idx_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/bertsum/model.py\u001b[0m in \u001b[0;36mbeam_decode_step\u001b[0;34m(inst_dec_beams, len_dec_seq, src_seq, enc_output, inst_idx_to_position_map, beam_size)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mdec_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_beam_dec_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst_dec_beams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_dec_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mword_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_active_inst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# Update the beam with predicted word prob information and collect incomplete instances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/bertsum/model.py\u001b[0m in \u001b[0;36mpredict_word\u001b[0;34m(dec_seq, src_seq, enc_output, n_active_inst, beam_size)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mpredict_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_active_inst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mdec_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m                 \u001b[0mdec_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Pick the last step: (bh * bm) * d_h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mword_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/bertsum/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt_seq, src_seq, enc_output)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mdec_enc_attn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_attn_key_pad_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_q\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mtgt_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;31m# -- Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mdec_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_seq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlMFUL-HtJuR",
        "colab_type": "code",
        "outputId": "db9da36d-812c-4d78-ce87-0e02783a696c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6798116839783025"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpn-Cj9ms3hf",
        "colab_type": "code",
        "outputId": "ea8ba79a-01b2-4187-c09a-b6d7a9859856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(hyp_list), len(ref_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13408 13408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M85q_0yll7_",
        "colab_type": "text"
      },
      "source": [
        "##Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3rb1QF9mMrB",
        "colab_type": "text"
      },
      "source": [
        "###Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvn5wKk-mFlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ARGS(object):\n",
        "    # data_dir = 'data/processed_data'\n",
        "    bert_model = 'bert-base-uncased'\n",
        "    output_dir = 'output'\n",
        "    model_path =  'output/model_01-13-05:43:53/BertAbsSum_1.bin'\n",
        "    config_path = 'output/model_01-13-05:43:53/config.json'\n",
        "    result_path = 'result'\n",
        "    batch_size = 16\n",
        "    max_src_len = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W46pBZXuX6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "with open(args.config_path, 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "model = BertAbsSum(args.bert_model, config['decoder_config'], device)\n",
        "model.load_state_dict(torch.load(args.model_path))\n",
        "model.to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gio6mNTmmQR-",
        "colab_type": "text"
      },
      "source": [
        "###Generate titles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9Bti4I4ljz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "# test = pd.read_csv('../data/test.csv', encoding='utf8')\n",
        "\n",
        "titles = []\n",
        "\n",
        "for row in tqdm(test.iterrows(), desc=\"Iteration\", total = test.shape[0], position=0):\n",
        "  i, text = row\n",
        "  src, src_mask = convert_one_example(text[0], args.max_src_len, tokenizer)\n",
        "  pred, _ = model.beam_decode(src.to(device), src_mask.to(device), 3, 3)  \n",
        "  # print(\" \".join(tokenizer.convert_ids_to_tokens(pred[0][0])).split('[SEP]')[0])\n",
        "\n",
        "  # De-tokenize.\n",
        "  tok_text = \" \".join(tokenizer.convert_ids_to_tokens(pred[0][0])).split('[SEP]')[0]\n",
        "  tok_text = tok_text.replace(\" ##\", \"\")\n",
        "  tok_text = tok_text.replace(\"##\", \"\")  \n",
        "  \n",
        "  # tok_text = ''\n",
        "  # for t in tokenizer.convert_ids_to_tokens(pred[0][0]):\n",
        "  #   tok_text += ' ' + t if not t.startswith('##') else t[2:]\n",
        "  # tok_text = tok_text.split('[SEP]')[0][1:]\n",
        "  titles.append(tok_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUli0ZLWljxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_df = pd.DataFrame({'abstract': test.abstract, 'title': titles})\n",
        "submission_df.to_csv('predicted_titles.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVgoREBHmV5L",
        "colab_type": "text"
      },
      "source": [
        "###Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhR5PhD9l1Hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "from nltk.util import ngrams\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "\n",
        "def generate_csv(input_file='predicted_titles.csv',\n",
        "                 output_file='submission.csv',\n",
        "                 voc_file='../data/vocs.pkl'):\n",
        "    '''\n",
        "    Generates file in format required for submitting result to Kaggle\n",
        "    \n",
        "    Parameters:\n",
        "        input_file (str) : path to csv file with your predicted titles.\n",
        "                           Should have two fields: abstract and title\n",
        "        output_file (str) : path to output submission file\n",
        "        voc_file (str) : path to voc.pkl file\n",
        "    '''\n",
        "    data = pd.read_csv(input_file)\n",
        "    with open(voc_file, 'rb') as voc_file:\n",
        "        vocs = pickle.load(voc_file)\n",
        "\n",
        "    with open(output_file, 'w') as res_file:\n",
        "        res_file.write('Id,Predict\\n')\n",
        "        \n",
        "    output_idx = 0\n",
        "    for row_idx, row in data.iterrows():\n",
        "        trg = row['title']\n",
        "        trg = trg.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
        "        trg.extend(['_'.join(ngram) for ngram in list(ngrams(trg, 2)) + list(ngrams(trg, 3))])\n",
        "        \n",
        "        VOCAB_stoi = vocs[row_idx]\n",
        "        trg_intersection = set(VOCAB_stoi.keys()).intersection(set(trg))\n",
        "        trg_vec = np.zeros(len(VOCAB_stoi))    \n",
        "\n",
        "        for word in trg_intersection:\n",
        "            trg_vec[VOCAB_stoi[word]] = 1\n",
        "\n",
        "        with open(output_file, 'a') as res_file:\n",
        "            for is_word in trg_vec:\n",
        "                res_file.write('{0},{1}\\n'.format(output_idx, int(is_word)))\n",
        "                output_idx += 1\n",
        "\n",
        "\n",
        "generate_csv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHHG7mRcl1OO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIF7JirNl1Ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjVt5fNxmcUp",
        "colab_type": "text"
      },
      "source": [
        "#Trash"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZS3pEsLljtm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l-ynDMcvOlD",
        "colab_type": "code",
        "outputId": "e8d0b5b8-08a7-4518-d83a-be47eb21fb33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "test_examples = processor.get_test_examples('../data/test.csv')\n",
        "\n",
        "test_features = convert_examples_to_features(test_examples, args.max_src_len, args.max_tgt_len, tokenizer)\n",
        "test_data = create_dataset(test_features)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=args.batch_size, drop_last=True)\n",
        "logger.info('Loading test data complete.')\n",
        "\n",
        "\n",
        "#test_dataloader"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-d60b8b9544ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_test_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_examples_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_src_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_tgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ARGS' object has no attribute 'max_src_len'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eaL_85SvK9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for batch in tqdm(test_dataloader, desc=\"Iteration\", position=0):\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    pred, _ = model.beam_decode(batch[0], batch[1], 3, 3)\n",
        "    src, tgt = batch[0], batch[2]\n",
        "    for i in range(args.batch_size):\n",
        "        sample_src = \"\".join(tokenizer.convert_ids_to_tokens(src[i].cpu().numpy())).split('[CLS]')[1].split('[SEP]')[0] + '\\n'\n",
        "        sample_tgt = \"\".join(tokenizer.convert_ids_to_tokens(tgt[i].cpu().numpy())).split('[CLS]')[1].split('[SEP]')[0] + '\\n'\n",
        "        sample_pred = \"\".join(tokenizer.convert_ids_to_tokens(pred[i][0])).split('[SEP]')[0] + '\\n'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13eh_d6k0r4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "from nltk.util import ngrams\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "with open('../data/vocs.pkl', 'rb') as voc_file:\n",
        "    vocs = pickle.load(voc_file)\n",
        "\n",
        "\n",
        "output_idx = 0\n",
        "for row_idx, text in test[:10].iterrows():\n",
        "  \n",
        "  src, src_mask = convert_one_example(text[0], args.max_src_len, tokenizer)\n",
        "  pred, _ = model.beam_decode(src.to(device), src_mask.to(device), 3, 3)\n",
        "  trg = \"\".join(tokenizer.convert_ids_to_tokens(pred[0][0])).split('[SEP]')[0]\n",
        "  # trg = row['title']\n",
        "  print(trg)\n",
        "  trg = trg.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
        "  print(trg)\n",
        "  trg.extend(['_'.join(ngram) for ngram in list(ngrams(trg, 2)) + list(ngrams(trg, 3))])\n",
        "  \n",
        "  VOCAB_stoi = vocs[row_idx]\n",
        "  trg_intersection = set(VOCAB_stoi.keys()).intersection(set(trg))\n",
        "  trg_vec = np.zeros(len(VOCAB_stoi))    \n",
        "\n",
        "  for word in trg_intersection:\n",
        "      trg_vec[VOCAB_stoi[word]] = 1\n",
        "\n",
        "  for is_word in trg_vec:\n",
        "      #res_file.write('{0},{1}\\n'.format(output_idx, int(is_word)))\n",
        "      print('{0},{1}\\n'.format(output_idx, int(is_word)))\n",
        "      output_idx += 1\n",
        "\n",
        "  print('')      \n",
        "\n",
        "  # with open(output_file, 'a') as res_file:\n",
        "  #     for is_word in trg_vec:\n",
        "  #         res_file.write('{0},{1}\\n'.format(output_idx, int(is_word)))\n",
        "  #         output_idx += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ64P3nB-ZEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "titles = []\n",
        "for abstract in abstracts:\n",
        "    title, _ = translate_sentence(model, abstract.split())\n",
        "    titles.append(' '.join(title).replace('<unk>', ''))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV_z48okln0y",
        "colab_type": "code",
        "outputId": "b6c98501-22b7-43c2-d6c1-f2b4e90b187c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "# test = pd.read_csv('../data/test.csv', encoding='utf8')\n",
        "\n",
        "titles = []\n",
        "\n",
        "for row in tqdm(test.iterrows(), desc=\"Iteration\", total = test.shape[0], position=0):\n",
        "  i, text = row\n",
        "  src, src_mask = convert_one_example(text[0], args.max_src_len, tokenizer)\n",
        "  pred, _ = model.beam_decode(src.to(device), src_mask.to(device), 3, 3)\n",
        "  # print(\" \".join(tokenizer.convert_ids_to_tokens(pred[0][0])).split('[SEP]')[0])\n",
        "  s = ''\n",
        "  for t in tokenizer.convert_ids_to_tokens(pred[0][0]):\n",
        "    s += ' ' + t if not t.startswith('##') else t[2:]\n",
        "  s = s.split('[SEP]')[0][1:]\n",
        "  titles.append(s)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 1000it [05:12,  2.82it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1K1X7B2-vhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_df = pd.DataFrame({'abstract': test.abstract, 'title': titles})\n",
        "submission_df.to_csv('predicted_titles.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGDsz4Pp0VNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HMaLLNXBH-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}