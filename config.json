{
	"bert_config": {
		"attention_probs_dropout_prob": 0.1, 
		"hidden_act": "gelu", 
		"hidden_dropout_prob": 0.1, 
		"hidden_size": 768, 
		"initializer_range": 0.02, 
		"intermediate_size": 3072, 
		"max_position_embeddings": 512, 
		"num_attention_heads": 12, 
		"num_hidden_layers": 12, 
		"type_vocab_size": 2, 
		"vocab_size": 30522
	}, 
	"decoder_config": {
		"len_max_seq": 30, 
		"d_word_vec": 768, 
		"n_layers": 8, 
		"n_head": 12, 
		"d_k": 64, 
		"d_v": 64, 
		"d_model": 768, 
		"d_inner": 768, 
		"vocab_size": 30522
	}
}