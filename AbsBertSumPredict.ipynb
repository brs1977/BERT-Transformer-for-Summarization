{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AbsBertSumPredict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkFqFkLzzCPR/3V4TP6sIh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brs1977/BERT-Transformer-for-Summarization/blob/master/AbsBertSumPredict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wRWZyBmvdMU",
        "colab_type": "text"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOT12ndrnxEd",
        "colab_type": "code",
        "outputId": "ef477721-4744-4408-ad8e-e59f302558e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/brs1977/BERT-Transformer-for-Summarization.git bertsum\n",
        "%cd /content/bertsum\n",
        "\n",
        "class ARGS(object):\n",
        "    bert_model = 'bert-base-uncased'\n",
        "    model_path =  'https://drive.google.com/uc?id=1-QRy87UMkoPCPVQrNbroC-XrIo1Qmj--&export=download'\n",
        "    config_path = 'https://drive.google.com/uc?id=1-0QeYVALlC6iOlN8XEU6SyI6DUd50O5a&export=download'\n",
        "    # batch_size = 16\n",
        "    max_src_len = 130\n",
        "\n",
        "args = ARGS()    \n",
        "\n",
        "#download model and config\n",
        "!gdown $args.model_path\n",
        "!gdown $args.config_path\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "fatal: destination path 'bertsum' already exists and is not an empty directory.\n",
            "/content/bertsum\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-QRy87UMkoPCPVQrNbroC-XrIo1Qmj--\n",
            "To: /content/bertsum/BertAbsSum_22.bin\n",
            "816MB [00:04, 178MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-0QeYVALlC6iOlN8XEU6SyI6DUd50O5a\n",
            "To: /content/bertsum/config.json\n",
            "100% 467/467 [00:00<00:00, 385kB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqXF27fgNeMf",
        "colab_type": "text"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn3YCmN-oCix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from model import BertAbsSum\n",
        "import json\n",
        "from preprocess import convert_examples_to_features\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "\n",
        "\n",
        "#create tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#load config\n",
        "with open('/content/bertsum/config.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "#create model\n",
        "model = BertAbsSum(args.bert_model, config['decoder_config'], device)\n",
        "model.load_state_dict(torch.load('/content/bertsum/BertAbsSum_22.bin'))\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "def convert_one_example(text, src_max_seq_length, tokenizer):\n",
        "    src_tokens = tokenizer.tokenize(text)\n",
        "    if len(src_tokens) > src_max_seq_length - 2:\n",
        "        src_tokens = src_tokens[:(src_max_seq_length - 2)]\n",
        "    src_tokens = [\"[CLS]\"] + src_tokens + [\"[SEP]\"]\n",
        "\n",
        "    src_ids = tokenizer.convert_tokens_to_ids(src_tokens)\n",
        "\n",
        "    src_mask = [1] * len(src_ids)\n",
        "    src_padding = [0] * (src_max_seq_length - len(src_ids))\n",
        "    src_ids += src_padding\n",
        "    src_mask += src_padding\n",
        "\n",
        "    return torch.tensor([src_ids]), torch.tensor([src_mask])    \n",
        "\n",
        "def predict(predict_text, beam_size=5, n_best=5):\n",
        "  \"\"\"predict function\"\"\"\n",
        "  src, src_mask = convert_one_example(predict_text, args.max_src_len, tokenizer)\n",
        "  pred, _ = model.beam_decode(src.to(device), src_mask.to(device), beam_size=beam_size, n_best=n_best)  \n",
        "\n",
        "  # De-tokenize.\n",
        "  tok_text = \" \".join(tokenizer.convert_ids_to_tokens(pred[0][0])).split('[SEP]')[0]\n",
        "  tok_text = tok_text.replace(\" ##\", \"\")\n",
        "  return tok_text.replace(\"##\", \"\")  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z-AU_n7Ngcu",
        "colab_type": "text"
      },
      "source": [
        "#Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH-LEdFboClz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#predict\n",
        "predict_text = '''Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings.'''\n",
        "predict(predict_text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}