{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AbsBertSumPredict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4Eiz/yk8dLzT/ZzqqkBDu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brs1977/BERT-Transformer-for-Summarization/blob/master/AbsBertSumPredict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wRWZyBmvdMU",
        "colab_type": "text"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOT12ndrnxEd",
        "colab_type": "code",
        "outputId": "e4dbfb34-0641-4b35-bf46-0905dd79b96b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/brs1977/BERT-Transformer-for-Summarization.git bertsum\n",
        "%cd /content/bertsum\n",
        "\n",
        "class ARGS(object):\n",
        "    bert_model = 'bert-base-uncased'\n",
        "    model_path =  'https://drive.google.com/uc?id=1-AhwiqfWWSLwJuvUo7XpR8vfMd-g9Tw9&export=download' #'https://drive.google.com/uc?id=1-QRy87UMkoPCPVQrNbroC-XrIo1Qmj--&export=download'\n",
        "    # config_path = 'https://drive.google.com/uc?id=1-0QeYVALlC6iOlN8XEU6SyI6DUd50O5a&export=download'\n",
        "    # batch_size = 16\n",
        "    max_src_len = 130\n",
        "\n",
        "args = ARGS()    \n",
        "\n",
        "#download model and config\n",
        "!gdown $args.model_path\n",
        "# !gdown $args.config_path\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'bertsum'...\n",
            "remote: Enumerating objects: 131, done.\u001b[K\n",
            "remote: Counting objects: 100% (131/131), done.\u001b[K\n",
            "remote: Compressing objects: 100% (102/102), done.\u001b[K\n",
            "remote: Total 131 (delta 61), reused 80 (delta 28), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (131/131), 148.42 KiB | 868.00 KiB/s, done.\n",
            "Resolving deltas: 100% (61/61), done.\n",
            "/content/bertsum\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-AhwiqfWWSLwJuvUo7XpR8vfMd-g9Tw9\n",
            "To: /content/bertsum/BertAbsSum_3.bin\n",
            "816MB [00:11, 70.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqXF27fgNeMf",
        "colab_type": "text"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn3YCmN-oCix",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "outputId": "60809132-0132-4a26-8cb1-5fa773039464"
      },
      "source": [
        "import torch\n",
        "from model import BertAbsSum\n",
        "import json\n",
        "from preprocess import convert_examples_to_features\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "\n",
        "\n",
        "#create tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#load config\n",
        "with open('/content/bertsum/config.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "#create model\n",
        "model = BertAbsSum(args.bert_model, config['decoder_config'], device)\n",
        "model.load_state_dict(torch.load('/content/bertsum/BertAbsSum_3.bin'))\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "def convert_one_example(text, src_max_seq_length, tokenizer):\n",
        "    src_tokens = tokenizer.tokenize(text)\n",
        "    if len(src_tokens) > src_max_seq_length - 2:\n",
        "        src_tokens = src_tokens[:(src_max_seq_length - 2)]\n",
        "    src_tokens = [\"[CLS]\"] + src_tokens + [\"[SEP]\"]\n",
        "\n",
        "    src_ids = tokenizer.convert_tokens_to_ids(src_tokens)\n",
        "\n",
        "    src_mask = [1] * len(src_ids)\n",
        "    src_padding = [0] * (src_max_seq_length - len(src_ids))\n",
        "    src_ids += src_padding\n",
        "    src_mask += src_padding\n",
        "\n",
        "    return torch.tensor([src_ids]), torch.tensor([src_mask])    \n",
        "\n",
        "def predict(predict_text, beam_size=5, n_best=5):\n",
        "  \"\"\"predict function\"\"\"\n",
        "  src, src_mask = convert_one_example(predict_text, args.max_src_len, tokenizer)\n",
        "  pred, _ = model.beam_decode(src.to(device), src_mask.to(device), beam_size=beam_size, n_best=n_best)  \n",
        "\n",
        "  # De-tokenize.\n",
        "  tok_text = \" \".join(tokenizer.convert_ids_to_tokens(pred[0][0])).split('[SEP]')[0]\n",
        "  tok_text = tok_text.replace(\" ##\", \"\")\n",
        "  return tok_text.replace(\"##\", \"\")  "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/11/2020 08:27:46 - INFO - pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmpa3brnu7b\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 932242.95B/s]\n",
            "02/11/2020 08:27:46 - INFO - pytorch_pretrained_bert.file_utils -   copying /tmp/tmpa3brnu7b to cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "02/11/2020 08:27:46 - INFO - pytorch_pretrained_bert.file_utils -   creating metadata file for /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "02/11/2020 08:27:46 - INFO - pytorch_pretrained_bert.file_utils -   removing temp file /tmp/tmpa3brnu7b\n",
            "02/11/2020 08:27:46 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "02/11/2020 08:27:47 - INFO - pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz not found in cache, downloading to /tmp/tmpniodx75k\n",
            "100%|██████████| 407873900/407873900 [00:13<00:00, 30326000.04B/s]\n",
            "02/11/2020 08:28:01 - INFO - pytorch_pretrained_bert.file_utils -   copying /tmp/tmpniodx75k to cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "02/11/2020 08:28:02 - INFO - pytorch_pretrained_bert.file_utils -   creating metadata file for /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "02/11/2020 08:28:02 - INFO - pytorch_pretrained_bert.file_utils -   removing temp file /tmp/tmpniodx75k\n",
            "02/11/2020 08:28:02 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "02/11/2020 08:28:02 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpz6tgdhca\n",
            "02/11/2020 08:28:06 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z-AU_n7Ngcu",
        "colab_type": "text"
      },
      "source": [
        "#Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH-LEdFboClz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04ddb08c-92c8-457f-c264-e355b8d22ebf"
      },
      "source": [
        "\n",
        "#predict\n",
        "predict_text = '''Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings.'''\n",
        "predict(predict_text)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tensor - level encoderal encoder for text summarization '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcODQNJ1Xm_k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "44e4b969-092c-479b-be55-ff4e17613f9a"
      },
      "source": [
        "predict_text = \"we consider the problem of utility maximization for investors with power utility functions. building on the earlier work larsen et al. (2016), we prove that the value of the problem is a frechet-differentiable function of the drift of the price process, provided that this drift lies in a suitable banach space.   we then study optimal investment problems with non-markovian driving processes. in such models there is no hope to get a formula for the achievable maximal utility. applying results of the first part of the paper we provide first order expansions for certain problems involving fractional brownian motion either in the drift or in the volatility. we also point out how asymptotic results can be derived for models with strong mean reversion.\"\n",
        "print(predict(predict_text))\n",
        "'on optimal investment with processes of long or negative memory'\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "on optimal investment with fractional levy processes \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'on optimal investment with processes of long or negative memory'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v32t7253XnDL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "735f76ec-302a-45dc-8c24-a1d370e606b2"
      },
      "source": [
        "predict_text = \"in this paper we provide an explicit formula for calculating the boolean number of a ferrers graph. by previous work of the last two authors, this determines the homotopy type of the boolean complex of the graph. specializing to staircase shapes, we show that the boolean numbers of the associated ferrers graphs are the genocchi numbers of the second kind, and obtain a relation between the legendre-stirling numbers and the genocchi numbers of the second kind. in another application, we compute the boolean number of a complete bipartite graph, corresponding to a rectangular ferrers shape, which is expressed in terms of the stirling numbers of the second kind. finally, we analyze the complexity of calculating the boolean number of a ferrers graph using these results and show that it is a significant improvement over calculating by edge recursion.\"\n",
        "print(predict(predict_text))\n",
        "'boolean complexes for ferrers graphs'\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "boolean complexes for ferrers graphs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'boolean complexes for ferrers graphs'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKdjiHRxXnFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a7a65aa4-30aa-4f0f-86bc-2baae4ec3235"
      },
      "source": [
        "predict_text = \"kinesin-5, also known as eg5 in vertebrates is a processive motor with 4 heads, which moves on filamentous tracks called microtubules. the basic function of kinesin-5 is to slide apart two anti-parallel microtubules by simultaneously walking on both the microtubules. we develop an analytical expression for the steady-state relative velocity of this sliding in terms of the rates of attachments and detachments of motor heads with the atpase sites on the microtubules. we first analyse the motion of one pair of motor heads on one microtubule and then couple it to the motion of the other pair of motor heads of the same motor on the second microtubule to get the relative velocity of sliding.\"\n",
        "print(predict(predict_text))\n",
        "'relative velocity of sliding of microtubules by the action of kinesin-5'\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "relative velocity of sliding of the sliding of kinesin - 5 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'relative velocity of sliding of microtubules by the action of kinesin-5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    }
  ]
}